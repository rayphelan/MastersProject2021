\documentclass[oneside,12pt]{book}

% Package dependencies
\usepackage{geometry}
\usepackage{lipsum}
\geometry{left=32mm, right=30mm, bottom=25mm, top=25mm}
\usepackage{amsmath , amsthm , amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{apacite}
\usepackage{scrextend}
\usepackage{blindtext}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{csquotes}
\usepackage{bm}
\usepackage{subcaption}
\usepackage{tocbibind}
\usepackage{pdfpages}
\usepackage[toc,page]{appendix}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{multirow}

% document formatting
\addtokomafont{labelinglabel}{\sffamily}
\setlength{\columnsep}{1cm}
\renewcommand{\baselinestretch}{1.5}

\usepackage[american]{babel}

\usepackage{csquotes}
\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{bibs.bib}

% Hypotheses
\newtheorem{hypothesis}{H1 - Alternate Hypothesis}
\newtheorem{nullhypothesis}{H0 - Null Hypothesis}

% Code Listings
\usepackage{listings}

% Listing Styles
\lstdefinestyle{myStyle}{
  numbers=left,
  stepnumber=1,
  numbersep=10pt,
  tabsize=4,
  showspaces=false,
  showstringspaces=false
}

\begin{document}

% Title page
\begin{titlepage}
    \begin{center}
        \vspace*{1.5cm}
        
        \Huge
        \textbf{An Evaluation on the Performance of Code Generated with WebAssembly Compilers}
        
        \vspace{0.5cm}
        \begin{figure}[H]
    	\centering
    	\hspace{7mm} \includegraphics[scale=0.5]{TU_logo}
        \end{figure}
        
        \vspace{1.5cm}
        
        \textbf{Raymond Phelan}
        
        \vfill
       \large
        A dissertation submitted in partial fulfilment of the requirements of\\
	Dublin Institute of Technology for the degree of\\
	M.Sc. in Computing (TU060)\\
       \vspace{0.5cm}
        \textbf{Date}
        \vspace{0.8cm}
 
    \end{center}
\end{titlepage}

% Define headers / footer style for the whole doc
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[CE,CO]{\thepage}
\renewcommand{\headrulewidth}{0pt}

% APA style for referencing
\bibliographystyle{apacite}

% Roman numerals for the 'administrative' sections
\pagenumbering{Roman}

% Declaration page
\chapter*{Declaration}
\addcontentsline{toc}{chapter}{Declaration}
I certify that this dissertation which I now submit for examination for the award of
MSc in Computing (Stream), is entirely my own work and has not been taken
from the work of others save and to the extent that such work has been cited and
acknowledged within the text of my work.
\\
\\
This dissertation was prepared according to the regulations for postgraduate study of
the Dublin Institute of Technology and has not been submitted in whole or part for an
award in any other Institute or University.
\\
\\
The work reported on in this dissertation conforms to the principles and requirements
of the Instituteâ€™s guidelines for ethics in research.
\vfill
\noindent
\textit{\textbf{Signed:}}  \\

\noindent
\textit{\textbf{Date:}}
\vspace{0.8cm}

% Abstract
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
\par WebAssembly is a new technology that is revolutionizing the web. Essentially it is a low-level binary instruction set that can be run on browsers, servers or stand-alone environments. Many programming languages either currently have, or are working on, compilers that will compile the language into WebAssembly. This means that applications written in languages like C++ or Rust can now be run on the web, directly in a browser or other environment. However, as we will highlight in this research, the quality of code generated by the different WebAssembly compilers varies and causes performance issues.

This research paper aims to evaluate the code generated by a number of existing WebAssembly compilers in order to determine whether or not there is a significant difference in their performances regarding execution times.



\\
\vfill
\noindent
\textbf{Keywords:} \quad WebAssembly, WebAssembly Compilers, Benchmarking Performance, WASM, WAT, AssemblyScript, C/C++, Rust

%Acknowledgements
\chapter*{Acknowledgments}
\addcontentsline{toc}{chapter}{Acknowledgments}

I would like to thank my supervisor, Paul Kelly, for his on going support and advise throughout this dissertation. I also express my sincere gratitude to all the lecturers in TU Dublin that I have had the pleasure of attending their classrooms.

I would also like to say a very special thank you to, my mother June Phelan, my father Raymond Patrick Phelan, my wife Diana Phelan and my children, Lucas Perico Phelan, Tawana Akinlolu and Tamiya June Phelan, for their constant encouragement and support.

Finally, I would like to thank my classmates that I have had the pleasure of getting to know along this journey.

% Contents and lists
\newpage
\tableofcontents

\listoffigures

\listoftables

\lstlistoflistings

\chapter*{List of Acronyms}
\addcontentsline{toc}{chapter}{List of Acronyms}
\begin{table}[H]
  \centering
    \begin{tabular}{ l l }
    \textbf{WASM} & Extension for compiled WebAssembly files \\
    \textbf{WAT} & Extension for human readable format of WebAssembly \\ 
    \textbf{AS} & AssemblyScript \\
    \textbf{VM} & Virtual Machine \\
    \textbf{EVM} & Ethereum Virtual Machine
    \end{tabular}
\end{table}

\newpage

% Standard numbering starts from here
\pagenumbering{arabic}
\fancyhead[RE,LO]{\leftmark}
\renewcommand{\headrulewidth}{2pt}


% Start of core thesis content

%Introduction chapter
% ============================================================= %
\chapter{Introduction}
\section{Background}

WebAssembly, also known as WASM, is a new technology revolutionizing the web. It is a low level binary instruction set, similar to pure assembly language, designed to run at near native speeds otherwise only achieved with C code. It can be run on the client-side in web browsers, on the server-side with NodeJS, and even as a stand-alone environment beyond the web, known as WASI (WebAssembly System Interface) \parencite[]{noauthor_wasi_nodate, noauthor_standardizing_nodate, noauthor_bytecodealliancewasmtime_2021}.

WebAssembly was first announced on December 5th, 2019, by the World Wide Web Consortium (W3C). For many years, JavaScript has been the only option for providing interactive applications to websites \parencite{Musch2019}, although there have been many attempts to remedy this, for example Adobe Flash and Microsoft Active X. However, these needed to be installed as browser extensions which prevented them from large scale acceptance. WebAssembly on the other hand, requires no extensions to be installed and is already supported by all of the major web browsers.

Programs written in statically typed languages such as C, C++, and Rust can now be compiled into WebAssembly. As JavaScript is a dynamically typed language, it cannot be directly compiled into WebAssembly. TypeScript however, a subset of the JavaScript language, can be compiled into WebAssembly. Many other programming languages are working towards enabling compilation to WebAssembly \parencite{appcypher_appcypherawesome-wasm-langs_2021}. This opens the door for many new possibilities as developers from these languages can now develop for the web, where previously this was not possible.

As WebAssembly is essentially binary code, this cannot be easily understood by human readers. There is however a textual representation of WebAssembly, known as WAT \parencite{noauthor_wat_nodate}. There are tools available for converting pure WebAssembly binary code into this text format and vice-versa, such as the WABT tool \parencite{noauthor_webassemblywabt_2021}. C and C++ can be compiled to WebAssembly using the Emscripten compiler \parencite{emscripten} and Rust can be compiled using the wasm-pack compiler \parencite{noauthor_wasm-pack_nodate}. AssemblyScript \parencite{noauthor_assemblyscript_nodate} is a variant of TypeScript which can compile directly to WebAssembly. These compilers will be the focus of this research project.

\section{Research Project/problem}

As more and more WebAssembly compilers emerge, we can expect to see a large variety of applications developed in multiple programming languages and compiled into WebAssembly. However, these compilers do not necessarily produce the same binary code for a given program written in different programming languages, as we will demonstrate later in this paper. In fact, even for a very simple program, such as a function to calculate the Fibonacci sequence, the different compilers are producing very different WebAssembly binary format code. 

These differences can be visually verified by inspecting a human readable format of the WebAssembly function in the text format, WAT. Since the WebAssembly compilers are producing vastly different code between them, even if they are performing the same function, this may lead to questions such as - which compiler generated the correct code? Do the compiled WebAssembly modules all have the same performances, or do they vary in efficiency in terms of execution times and file sizes? 

Initially one might have thought that once compiled into WebAssembly, there would be little or no difference in the overall performance between the WebAssembly modules compiled with difference compilers. However, if they have significantly different performances, even though they are all running as WebAssembly files, this means that it is important to understand which programming language and respective compiler developers should choose before developing WebAssembly applications.

These concerns will be the purpose and focus of this research paper. We propose the following research question:

\begin{quote}\emph{Is there a significant difference in the performance of WebAssembly modules, in terms of execution times, for the same program written in different programming languages and compiled into WebAssembly using their respective compilers?}\end{quote}


\section{Research Objectives}

The main goal of this research is to determine whether or not there is a significant difference in performance of WebAssembly modules generated from different compilers. As we will discuss later in the literature review, at the time of writing this paper, no other research was found that answered our proposed research question. In order to answer the question, we identified the following research objectives:

\begin{itemize} 
\item Set up a virtual machine environment for running our experiments
\item Identify programming languages that can be compiled into WebAssembly
\item Set up the respective WebAssembly compilers for each of the selected programming languages
\item Identify and code relevant functions and algorithms that can be written in the selected programming languages and that can be properly compiled into WebAssembly
\item Create a benchmark runner for executing the WebAssembly modules from each of the selected programming languages and gather their execution times for post-analysis
\item Perform statistical analysis on the gathered execution times in order to answer the research question
\end{itemize}

\section{Research Methodologies}

The type of research we will perform is considered a primary research, since the data that will be used in our experiments does not exist currently in the reviewed literature. The data will be generated by executing the WebAssembly modules in a single environment and gathering their execution times. 

In terms of the research objective and whether it implies quantitative or qualitative research, we will be performing our statistical analysis based on numerical data representing the execution times of the WebAssembly modules. Therefore our research can be considered to be a quantitative research.

The form of research we are conducting can be considered to be an empirical research, since we will be making direct observations on the data collected through our scientific experiment. We will also be defining the null and alternative hypotheses based on our research question.

Finally, the reasoning involved in our research can be considered to be deductive reasoning, since we will be answering the research question by testing the hypotheses through a process of evaluating the statistical data obtained during our experiments. We will then draw conclusions to our research question based on the outcome of testing the null and alternative hypotheses.


\section{Scope and Limitations}

The aim of this research paper is to compare the performance of WebAssembly modules generated by different compilers for different programming languages. Our choice of programming languages was influenced by the current available support for each language. We ultimately chose C, C++, Rust and AssemblyScript for our experiments as they provided enough documentation and support for implementing their respective compilers. \Textcite{Long2021} also came to the same conclusion regarding WebAssembly compilers with sufficient support for different programming languages.

Although currently many other programming languages actually have the capability of being compiled to WebAssembly, as discussed later in this paper, we found limited resources for implementing the selected benchmarking functions in those languages, and therefore chose benchmarking functions that could be easily written in all of the selected programming languages.

Another important aspect of this research paper worth highlighting is that we did not necessarily try to find the most efficient implementation for each benchmarking function. Once we were able to replicate the same function in all the selected programming languages, then we would be able to properly compare their performance, regardless of whether the functions were the most efficient implementation or not.

Also worth mentioning is that each WebAssembly compiler must be able to compile the functionality considering the current limitations of WebAssembly, which is considered to be still in infancy stages. Furthermore, this research is limited to execution of WebAssembly modules in a NodeJS environment, rather than the full spectrum of possible environments, such as web-browsers and IoT devices.


\section{Document Outline}

This research project will be divided into logical chapters. Chapter 1 provides some background knowledge on WebAssembly in general. Here we explain the research problem we have identified and present the research question. We also describe the research objectives and methodologies used. Finally we provide an insight into the scope and limitations of this research.

Chapter 2 will involve a review of the current literature based on relevant scientific publications in order to highlight the research gaps and importance of our research question. Chapter 3 describes the experiment design and set up used to gather the required data. Chapter 4 will provide an in-dept evaluation and discussion on the results gathered through our statistical tests. Finally, chapter 5 will provide a conclusion to this research, describing the impact of our contributions and outlining future work.


% Literature review chapter
% ============================================================= %
\chapter{Review of existing literature}

Although WebAssembly is still relatively new, it is rapidly becoming a highly researched topic in the last 18 months. Researchers have been evaluating the possible benefits and use cases of WebAssembly, while developers have been enjoying freedom of choice in the programming languages they use to develop their applications, using a WebAssembly compiler to achieve the near native speeds promised by WebAssembly. In this section, we will review currently available literature that carried out some benchmarking on the performance of WebAssembly, or that proposed WebAssembly as solution to improve some existing applications, compared with the traditional approaches. In doing so, we will reveal the gap in the literature that has led us to our research question, highlighting that not enough research and evaluation has been done on the performance of code generated by the various WebAssembly compilers. We highlight that choosing only one WebAssembly compiler to evaluate the performance of WebAssembly is insufficient.

\Textcite{Sandhu2018} compared the performance of JavaScript and WebAssembly against C for sparse matrix-vector multiplication. They discover a slowdown of 2.2x to 5.8x with JavaScript versus C, but WebAssembly performing similar or better at times when compared with C in the browser. They used the Emscripten compiler to generate the WebAssembly modules from C code for their experiments. However, only the one WebAssembly compiler was used, therefore it is unknown whether WebAssembly code generated from other compilers might have produced different results.

\Textcite{Sandhu2020} compared WebAssembly to native C code, again using sparse matrix computations. However, they developed the WebAssembly implementations by hand rather then using a compiler. This time they discover that WebAssembly executed in the Chrome browser has performance issues due to memory addressing in the x86 instruction set. Ultimately, this leaves a gap in the research, as perhaps it would be important to understand if various WebAssembly compilers might have produced more efficient WebAssembly code than the hand-written implementation.

\Textcite{234914} use the PolybenchC Benchmark Suite \parencite{noauthor_polybenchc_nodate} to compare WebAssembly-compiled Unix applications versus native speeds inside the browser. Their findings revealed that WebAssembly ran up to 55\% slower. However, the WebAssembly modules that were used in their experiments were generated from the Emscripten compiler only, so once again we do not know if WebAssembly from other compilers might result in different performances.

\Textcite{Haas2017} also compared the performance of WebAssembly using the PolybenchC Benchmark Suite on browser JavaScript engines, such as Mozilla SpiderMonkey, Google V8 and Microsoft Chakra. The generation of WebAssembly modules was done with the assistance of the OCaml Programming Language \parencite{noauthor_ocaml_nodate}, rather than using any compiler. Once again, there was no evaluation of the WebAssembly code in comparison to what other compilers would generate by implementing different programming languages.

\Textcite{Herrera2018} evaluate the performance of JavaScript and WebAssembly compared to native C code. They used five of the benchmarks identified in the Ostrich Benchmark Suite \parencite{Khan2015} for numerical computing, and compiled them into WebAssembly modules using Emscripten. Their comparisons were done between web browsers, IoT devices and NodeJS. Their findings once again show that WebAssembly comes close to native C performance, but is faster than JavaScript. However, these experiments also were limited by only evaluating the WebAssembly generated from one compiler, rather than multiple compilers.

\Textcite{Reiser2017} presented their own WebAssembly compiler, Speedy.js, which compiles JavaScript/TypeScript to WebAssembly, although their compiler uses the Emscripten runtime library. They use a number of computing algorithms, such as Fibonacci, Prime Number and Merge Sort, to evaluate the performance of TypeScript compared to the WebAssembly implementation generated by their proposed solution. Indeed they find WebAssembly performance to be faster than plain TypeScript, however only the WebAssembly from their compiler was evaluated, rather than including WebAssembly generated by other compilers.

\Textcite{Protzenko2019} also present their own compiler, which compiles Low*, a low-level subset of F* programming language, into WebAssembly. They develop an alternative to the JavaScript encryption library, LibSignal \parencite{noauthor_signalapplibsignal-protocol-javascript_2021}, which is used in many modern services such as WhatsApp, Skype and Signal for managing end-to-end encryption. They report no speed up between WebAssembly and the JavaScript library, mainly due to overhead between encoding and decoding between WebAssembly and JavaScript. They suggest that future JavaScript libraries should be designed to be more WebAssembly friendly. However, only WebAssembly modules generated from their own compiler were used in this evaluation, therefore it is unknown if other WebAssembly compilers might have produced more efficient WebAssembly code.

\Textcite{Watt2019} propose a similar solution to cryptography using WebAssembly, rather than JavaScript. However, they use a different approach, proposing CT-WASM as an extension to WebAssembly, or as a new programming language that is similar to TypeScript, which includes its own compiler for generating pure WebAssembly modules. However, the performance of their generated WebAssembly modules is not evaluated against WebAssembly modules generated from other compilers.

\Textcite{Mendki2020} proposed a WebAssembly Serverless Edge Computing as an alternative to the native container-based application. They developed a compute and memory intensive application with file I/O and image classification using Rust and compiled it into WebAssembly. They found that WebAssembly had a lower footprint than the container-based solution while also showing faster start-up times, but the runtimes were slower than the native container application. However, only WebAssembly compiled from Rust was evaluated in their proposal, therefore it is unknown how well WebAssembly compiled from other languages might have performed.

A WebAssembly solution to the stateless containers used by existing serverless platforms was published in \parencite{Shillaker2020}, providing isolated memory and CPU, with state sharing capabilities. Their solution used the LLVM compiler \parencite{noauthor_llvm_nodate} to translate applications into WebAssembly from multiple programming languages such as, C, C++, Python, JavaScript and TypeScript. Their evaluation reported a 2x performance speed-up with 10x less memory consumption using their proposal against dockerised containers. However, they also only used the one compiler for the generation of WebAssembly modules in their evaluations.

\Textcite{Jeong2019} proposed an edge computing framework for offloading JavaScript and WebAssembly state between mobile devices, edge computing devices and cloud services. This system saves the state of JavaScript workers, objects and WebAssembly functions, and enables the transfer and restoration of these between devices, resulting in an 8.4x speedup compared with only offloading pure JavaScript code. All the WebAssembly files used in their experiments were generated from C++ code using the Emscripten compiler. Once again, this highlights the gap of knowing how WebAssembly from other compilers would have performed in these experiments.

WebAssembly was proposed in \parencite{Long2021} as a lightweight alternative for serverless Function as a Service environments, that could enable fine grained resource consumption at runtime, as WebAssembly functions can be started and stopped on demand. They concluded that WebAssembly VMs outperform Docker containers and have a smaller footprint. However, only the Emscripten WebAssembly compiler was used to generate the WebAssembly bytecode for their evaluations.

\Textcite{Hall2019} also proposed WebAssembly as an alternative to traditional Dockerised containers for serverless functions in edge computing. They found that WebAssembly can provide many of the same isolation requirements, such as memory safety and security, while eliminating the cold start time penalty that traditional serverless containers incur. Once again, the WebAssembly modules used in their experiments were all generated from C++ code compiled into WebAssembly using the Emscripten compiler.

\Textcite{Koren2021} presented a proof-of-concept for a standalone WebAssembly development environment, capable of running on the edge and in the cloud. It features a built-in web-server and a browser-based Integrated Development Environment (IDE) as an alternative to running containerized microservices on low powered IoT devices with limited capabilities. Their evaluations were also done using WebAssembly modules generated from a single compiler, this time, the AssemblyScript compiler.

\Textcite{Tiwary2020} proposed an alternative to container-based serverless computing, addressing their cold-start problems and complicated architectures, while providing stateful memory and multi-tenancy isolation. Their proposal suggests WebAssembly to be executed directly on ring 0 with the serverless functions placed as close to the data as possible, providing CPU memory and filesystem isolation that is required for multi-tenancy in cloud computing. Their proposal was developed in Rust and compiled into WebAssembly using the Rust WebAssembly compiler, therefore once again, performance of WebAssembly is based on the code generated from a single WebAssembly compiler.

\parencite{DBLP:journals/corr/abs-2012-01032} evaluate the performance of Ethereum blockchain smart contracts using WebAssembly. They implement 12 benchmarks in Rust and compile to WebAssembly. They also use the emerging compiler SOLL \parencite{noauthor_second-statesoll_2021} that generates WebAssembly bytecode from Solidity \parencite{team_solidity_nodate}. Their experiments compare the performance of the WebAssembly driven blockchain against the Ethereum Virtual Machine (EVM) implementation, finding that WebAssembly driven blockchain does not perform as well as the EVM. However, they did not use the full range of available WebAssembly compilers by implementing their benchmarks in other programming languages, which perhaps might have produced different results for the evaluation of WebAssembly.

FAUST is a domain specific functional programming language used in audio processing and sound synthesis for applications such as synthesizers, musical instruments and sound effects used in concerts, artistic productions, education and research. \Textcite{Letz2018} propose an alternative by compiling FAUST into WebAssembly and performing benchmarks to compare the performance of WebAssembly with native versions. They found that WebAssembly ran up to 66\% slower than native C++ versions. Interestingly, they question the code quality that will eventually be generated by multiple WebAssembly compilers. However, their experiments only involved the WebAssembly modules generated by the Emscripten compiler.

\Textcite{Taheri2018} discuss how computer vision on the web suffers from performance issues due to limitations in JavaScript. They developed a WebAssembly implementation of  the OpenCV computer-vision library, which was initially implemented in C++. The library was compiled into WebAssembly using Emscripten and the performance was evaluated against the original implementation. Their findings were that WebAssembly did indeed run at close to native speeds, however only one WebAssembly compiler was used to generate the WASM bytecode.

This pattern of only using one WebAssembly compiler to benchmark the performance of WebAssembly code repeats itself through all the literature we reviewed. \Textcite{Murphy2020} only used the Clang compiler to evaluate WebAssembly for serverless computing. \Textcite{CabreraArteaga2020} proposed a toolchain for the superoptimization of WebAssembly binary code to improve execution time and overall file size. However, their experiments are limited to just C code using the Emscripten and LLVM compilers.

In this section we discussed some of the available literature that evaluated the performance of WebAssembly generated from programming languages such as C, C++, Rust and AssemblyScript using their respective compilers. However, none of these evaluations compared the code generated from these compilers against each other. As we highlight in this paper, each WebAssembly compiler generates different code even for simple programs, which produce significant differences in performance times.


% Design and methodology chapter
% ============================================================= %
\chapter{Experiment design and methodology}

The entire source code and results of our experiments are publicly available on a GitHub repository and can be located at: 

https://github.com/rayphelan/MastersProject2021 \\

After installing the WebAssembly compilers for each of the selected languages and their required dependencies, this experiment can be fully reproduced by cloning the Git repository on a local Virtual Machine and following the steps provided on the Github repository and in this paper.

\subsection{Hypotheses}

As an example, if we write a simple Fibonacci algorithm in C/C++, AssemblyScript and Rust, maintaining identical syntax between the languages, will they all have the same performance when compiled into WebAssembly and run on the same environment? In order to answer our research question using the scientific methods mentioned in chapter 1, we propose the following hypotheses:

\subsubsection{H0 - Null Hpyothesis:}

\begin{quote}\emph{There is no significant difference in the performance of WebAssembly modules, in terms of execution times, for the same program written in different programming languages and compiled into WebAssembly using their respective compilers.}\end{quote}

\subsubsection{H1 - Alternate Hpyothesis:}

\begin{quote}\emph{There is a significant difference in the performance of WebAssembly modules, in terms of execution times, for the same program written in different programming languages and compiled into WebAssembly using their respective compilers.}\end{quote}

An important assumption of our experiments is that each of the algorithms across the selected programming languages is provided with the same input parameters and that the output of these algorithms is also the same for each language. This ensures that the results of the benchmarks are comparable, as highlighted in \parencite{Khan2015}. For this reason, we have hardcoded the input parameters into each of the algorithms to ensure that each version of the algorithm in each language will have the same workload to operate on.

Furthermore, the objective of our experiments is not to benchmark the printing capabilities of each compiled WebAssembly algorithm, therefore only one single output is printed at the end of each algorithm execution for the purpose of visually confirming that the algorithm executed correctly.

\section{Experiment Set up}

In the following sections will provide details on the Virtual Machine set up, Software Installation, Programming Languages and Algorithms used to perform our experiments. We also provide details of how the benchmarks were executed and how the results were gathered.

\subsection{Virtual Machine and Host Environment}

The Virtual Machine (VM) was installed on top of a Windows desktop computer. The details of both are listed below.

Table \ref{tab:vm_setup} show the details of the VM used in the experiments. 
\begin{table}[H]
  \centering
    \begin{tabular}{| c | c |}
    \hline
    \textbf{Operating System} & Ubuntu 20.10 (64-bit)\\ \hline
    \textbf{RAM} & 12075 MB \\ \hline
    \end{tabular}
  \caption{Virtual Machine}
  \label{tab:vm_setup}
\end{table}

Table \ref{tab:windows_pc} show the details of the environment used for hosting the VM.
\begin{table}[H]
  \centering
    \begin{tabular}{| c | c |}
    \hline
    \textbf{Operating System} & Windows 10 Enterprise (64-bit)\\ \hline
    \textbf{RAM} & 32 GB \\ \hline
    \textbf{Processor} & Intel(R) Core(TM) i7-10610U CPU @ 1.80GHz 2.30 GHz \\ \hline
    \end{tabular}
  \caption{Host Environment}
  \label{tab:windows_pc}
\end{table}


\subsection{Software and Compilers}

The following section provides details on the software that was required. This includes details on the programming languages and the WebAssembly compilers used in the experiments.

\subsubsection{Emscripten}

Emscripten \parencite{emscripten} is the compiler used to generate the WebAssembly files from C/C++ code. Installing Emscripten has a number of prerequisites, such as GIT \parencite{git}, NodeJS (Version 15.14.0) \parencite{nodejs_nodejs_nodate} and Python3 \parencite{python} and the GCC Compiler for C and C++ \parencite{noauthor_gcc_nodate}.

\subsubsection{Rust}

Rust was downloaded and installed from \parencite{rust} and the documentation for setting up the Rust to WebAssemby compiler, wasm-pack \parencite{noauthor_wasm-pack_nodate}, was found at \parencite{rust_webassembly}.

\subsubsection{AssemblyScript}

Documentation for setting up the AssemblyScript WebAssembly compiler can be found at \parencite{noauthor_assemblyscript_nodate}.

\subsubsection{RStudio}

RStudio \parencite{noauthor_rstudio_nodate} was used to perform the statistical analysis of our results.

\section{Algorithms}

The algorithms used in this experiment were first sourced on the internet as C/C++ programs. Then the equivalent of these algorithms were written in AssemblyScript and Rust programming languages. The algorithms were then compiled to WebAssembly using their respective compilers. The selection of algorithms used includes searching, sorting and numerical computing algorithms.

In our experiments, some of the algorithms were given an array of 100,000 elements as their input. A JavaScript program was created which generated 3 versions of an array with 100,000 elements. Each element is a unique floating point number with 7 decimal places. The first version is a sorted array with 100,000 elements. The second version is the same array in reserve order. The third version is the array with the elements in random positions. The respective array for each experiment was copied directly into the algorithms as their input parameters and the same array was provided to all algorithms which required an array as the input parameter.

The selection of algorithms used in this research were influenced by other research papers that previously used these algorithms. The NQueens algorithm has been used in papers such as \parencite{Herrera2018} and \parencite{Khan2015}. Recursive functions, such as the Fibonacci algorithm, are used in the following benchmarking paper \parencite{VanEekelen2019}. \Textcite{Reiser2017} used a number of algorithms for their benchmarks, including the Prime Numbers, Merge Sort and Fibonacci algorithms. The Bubble Sort, Merge Sort, Fibonacci, NQueens, Prime Number were also used in previous versions of the Ostrich Benchmarking Suite \parencite{noauthor_sableostrich2_nodate}.

\subsection{Numerical Computing Algorithms}

The numerical computing algorithms used in this experiment are NQueens, Primary Number and Fibonacci number algorithms. 

\subsubsection{NQueens}

The 8 Queens Puzzle was originally published in 1848 \parencite{noauthor_eight_2021}. The objective is to place 8 Chess Queens on a chess board that is of size 8x8, in such a manor that no Queens are in direct threat from each other. Given the complexity of this puzzle when used with larger board sizes, it has been used many times as a software algorithm for benchmarking, known as the NQueens algorithm. 

The NQueens algorithm is part the Ostrich Benchmark Suite \parencite{ostrich} developed at the Sable Lab at McGill University. It has been a popular research topic in computer science research for many years \parencite[]{Guldal2016, Ayub2018, Alhassan2019}.

In our experiments, we implemented two versions of the NQueens algorithm. The first using board of 24x24, the second using a board of 27x27. The algorithm was implemented in all of the selected programming languages keeping the same code structure as much as possible. The output of the algorithms was the printed solution of the board.

The C/C++ version of this algorithm was sourced at \parencite{nqueens_code}. The NQueens algorithm has a complexity of $O({n}^{n})$.

\subsubsection{Prime Number}

An important topic in computer science and cryptography is Number Theory, where the Prime Number calculation plays an important role \parencite{ElhakeemAbdElnaby2021}. A Prime Number is a number that has only two factors, 1 and the number itself. In this experiment, the Prime Numbers algorithm will calculate every prime number from 2 until 100,000. The output will print the 100,000th prime number. The C/C++ version of this algorithm was sourced at \parencite{primenumber} and the code for the other selected languages was written from scratch. The Prime Numbers algorithm has a complexity of $O(\sqrt{}(n))$.

\subsubsection{Fibonacci}

The Fibonacci sequence is another well known computer algorithm used for benchmarking and was recently used in \parencite{Mendki2020} for benchmarking WebAssembly performance. It takes a number as the input parameter, and adds the previous two numbers together to produce the next Fibonacci number sequence of the given input. For our experiment, we used the recursive method to calculate the Fibonacci sequence. The algorithm will return the 45th Fibonacci sequence number. The Fibonacci algorithm has a complexity of $O(\log{}n)$.


\subsection{Sorting Algorithms}

Sorting plays a crucial role in many algorithms \parencite{Abhay2019}. The sorting algorithms used in this experiment are Bubble Sort, Heap Sort, Merge Sort, Selection Sort and Shell Sort. Each algorithm is given the same array of 100,000 elements as the input. Each element consists of a unique floating point number between 0 and 99,999 with 7 decimal places. The input array is sorted in reverse order and the output of the algorithms will produce a sorted array of the same elements.

\subsubsection{Bubble Sort}

The Bubble Sort algorithm compares two adjacent elements and swaps them around if they are not in the required order. The C/C++ version of this algorithm was sourced at \parencite{bubblesort} and the algorithms were written in the other languages from scratch. The Bubble Sort algorithm has a complexity of $O(n2)$.


\subsubsection{Heap Sort}

The Heap Sort algorithm is generally slower than other sorting algorithms and therefore not commonly used, however we wanted to include this inefficient sorting algorithm in our experiments as we believe it still provides interesting insights. The C/C++ version of this algorithm was sourced at \parencite{heapsort} and the algorithms were written in the other languages from scratch. The Heap Sort algorithm has a complexity of $O(n\log{}n)$.

\subsubsection{Merge Sort}

The Merge Sort algorithm follows the principle of Divide and Conquer. It works by dividing the problem into multiple sub-problems, solving each of the smaller sub-problems, and merging the results back together to generate the output. The C/C++ versions of this algorithm was sourced at \parencite{mergesort} and the algorithms were written in the other languages from scratch. The Merge Sort algorithm has a complexity of $O(n*\log{}n)$.

\subsubsection{Selection Sort}

The Selection Sort algorithm selects the smallest element from an array and places it at the beginning. It repeats this process until the results represent a sorted array. The C/C++ version of this algorithm was sourced at \parencite{selectionsort} and the algorithms were written in the other languages from scratch. The Selection Sort algorithm has a complexity of $O(n2)$.

\subsubsection{Shell Sort}

The Shell Sort algorithm sorts elements at different intervals, starting from the elements that are furthest apart from each other and repeating the process until all the elements are in a sorted order. The C/C++ version of this algorithm was sourced at \parencite{shellsort} and the algorithms were written in the other languages from scratch. The Shell Sort algorithm has a complexity of $O(n2)$.


\subsection{Searching Algorithms}

The most commonly known searching algorithms are the Binary Search and Linear Search algorithms \parencite{Jacob2018}, therefore we have implemented these algorithms in our experiments. The input for these algorithms consist of an array of 100,000 elements of unique floating point numbers between 0 and 99,999 with 7 decimal places. The algorithm will run a loop from 0 to 99,999 searching for the number of each iteration within the input array. The output will be the index of the array where the searched element was found on the last iteration.

\subsubsection{Binary Search}

The Binary Search algorithm assumes that the given array will in a sorted order. Given a sorted array, the algorithm divides the array in half by selecting the middle element of the array. It determines if the number being searched for is greater or less than the value of the middle element. This means that one half of the array can be ignored as the number being searched for must be in the other half. This process is repeated recursively until the number being searched for is found. The C/C++ code for this algorithm was sourced at \parencite{binarysearch}  and the algorithms were written in the other languages from scratch. The Binary Search has a complexity of $O(\log{}n)$.

\subsubsection{Linear Search}

The Linear Search algorithm does not require a sorted array. In this experiment, the Linear Search algorithm was given an array of 100,000 elements of unique floating point numbers between 0 and 99,999 with 7 decimal places and the array has been given a random ordering. The algorithm performs a simple search on every element in the array, starting from the first element and finishing on the last element. The C/C++ code for this algorithm was sourced at \parencite{linearsearch} and the algorithms were written in the other languages from scratch. The Linear Search has a complexity of $O({}n)$.

\section{Compiling to WebAssembly}

This section will provide details on how each WebAssembly compiler was used. Each of the compilers come with optimization level options. Some optimizations can be made which will reduce the overall code size. Other optimizations can be made which increase performance in execution time. In our experiments, we chose to optimize for execution time speed rather than code size.

\subsection{C/C++ to WebAssembly}

Emscripten is used to compile C and C++ into WebAssembly. It takes in a C program as an argument and generates a WebAsssembly file and a JavaScript glue file. Emscripten allows for different levels of optimization, ranging from no optimization to highly optimized. For our experiments, we chose the optimization level -O2 as this provided a combination of well optimized code with fast compile times. 

The optimization levels that Emscripten provides are listed in table \ref{tab:emscripten}.

\begin{table}[H]
  \centering
    \begin{tabular}{| c | c |}
    \hline
    \textbf{Optimization Level} & Description\\ \hline
    \textbf-O0 & No optimization \\ \hline
    \textbf-O1 & Low optimization, shorter compile time \\ \hline
    \textbf-O2 & Well optimized build \\ \hline
    \textbf-O3 & Highly optimized, longer compile time \\ \hline
    \textbf-Os & Highly optimized, reduced code size, longer compile time\\ \hline
    \end{tabular}
  \caption{Emscripten Optimization Levels}
  \label{tab:emscripten}
\end{table}

Emscripten also takes an arguement to modularize the code. This means that the compiler will generate the JavaScript glue file in such a way that allows it to be imported as a JavaScript module. The compiler puts all of the required JavaScript into a factory function which can be called on to create an instance of the WebAssembly module. The following code in \ref{lst:BinaryCWasm} shows how the WebAssembly file and the JavaScript glue file were generated using Emscripten. 

\begin{lstlisting}[caption={Compiling BinarySearch from C to WebAssembly},label={lst:BinaryCWasm},language=Emscripten,basicstyle=\scriptsize,style=myStyle]
emcc binarySearch.c -o binarySearch.js -O2 
    -s MODULARIZE -s "EXPORTED_FUNCTIONS=['_binarySearch']"
\end{lstlisting}


\subsection{AssemblyScript to WebAssembly}

AssemblyScript was designed especially to target WebAssembly but offers a familiar syntax for TypeScript and JavaScript developers. It uses Node Package Manager (NPM) \parencite{noauthor_npm_nodate} as the installer. The AssemblyScript compiler also comes with options for optimization. For our experiments we chose the optimization level 3, for speed rather than code size, like we did with the Emscripten optimization levels. The following code in \ref{lst:ASOpt} show how the WebAssembly and JavaScript files were generated using AssemblyScript.

\begin{lstlisting}[caption={AssemblyScript Optimization},label={lst:ASOpt},language=AssemblyScript,basicstyle=\scriptsize,style=myStyle]
npm run asbuild
\end{lstlisting}

\subsection{Rust to WebAssembly}

To compile Rust to WebAssembly, the wasm-pack compiler was used. We passed an option to the compiler to note that we were targetting the NodeJS environment. This customizes the JavaScript glue file so it can be easily run in NodeJS. Wasm-pack allows us to specify whether or not the build should be optimized for production, using the --release keyword. However if no keyword is provided, the --release profile is automatically used. This was the optimization levels used in our experiments.  The following code in \ref{lst:wasmpack} shows how the WebAssembly and JavaScript files were generated for Rust.

\begin{lstlisting}[caption={Rust to WebAssembly},label={lst:wasmpack},language=Rust,basicstyle=\scriptsize,style=myStyle]
wasm-pack build --target nodejs
\end{lstlisting}



\section{Benchmarking and Gathering Results}

WebAssembly modules are most commonly loaded via its JavaScript API \parencite{Hall2019}. The WebAssembly compilers used in our experiments generate the boilerplate JavaScript which can then be be converted into a JavaScript module, allowing it to be conveniently imported into other JavaScript functions. Inspired by the Ostrich Benchmark Suite \parencite{ostrich} which created a runner python for performing the benchmarks and recording the results, we created our own JavaScript runner which runs our experiments in NodeJS with the desired number of iterations and records the final results onto a CSV file for post processing.

Each compiler generates the WASM file and the JavaScript glue for loading and interacting with the WASM file. For each algorithm in each language, we created a new JavaScript runner which loads the JavaScript Glue file. Once the WebAssembly module has been loaded, the JavaScript runner executes a loop which runs the main function in the corresponding WebAssembly module. The number of iterations in the loop depends on how many times we want to perform the experiment. After each iteration, the execution time in nanoseconds is measured and converted into seconds and milliseconds before being saved to an array of results. After the final iteration, the array is saved into a CSV file.

\Textcite{Herrera2018} suggested running each experiment 30 times, \parencite{Mendki2020} ran their experiments 100 times, while the JetStream2 Benchmark \parencite{noauthor_jetstream_nodate} suggested running the experiments 120 times. Initially we only ran our experiments 30 times each. However, upon visual inspection of the distribution of the results using histograms, we found that our data was not normalised. We then proceeded to run the experiments 120 times each. However, even after running the experiments 120 times each, the results mainly did not have a normal distribution. This would influence our choice in statistical tools during our evaluation of the results.

Figure \ref{fig:JSRunner} shows a sequence diagram of the JavaScript Runner file we created. The WebAssembly module is first loaded into the JavaScript memory. The JavaScript then iterates through a loop for the number of times we defined that was required and executes the main function in the compiled WebAssembly. Note that the time for loading the WebAssembly module is not being measured, since we are only interested in the execution time.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{images/JSRunner.PNG}
    \caption{Experiment JavaScript Runner}
    \label{fig:JSRunner}
\end{figure}

Our JavaScript experiment runner was manually invoked using NodeJS command line for each algorithm in each programming language. When all of the experiments had been completed, we created Excel files for each language (C, C++, AssemblyScript, Rust). 

Each column represents the algorithm being used in the experiment. Each row represents the execution time of the algorithm for the number of iterations performed. The Excel files were then saved as CSV files for further processing in RStudio to perform the statistical analysis.

The following code in list\ref{lst:JSRunnerC} represents the JavaScript runner for the Fibonacci algorithm in C.

\begin{lstlisting}[caption={JavaScript runner for Fibonacci in C WebAssembly},label={lst:JSRunnerC},language=JavaScript,basicstyle=\scriptsize,style=myStyle]
const wasmModule = require('./fibonacci.js');
const fs = require('fs');
const results = [];
const iterations = 120;
wasmModule().then((instance) => {
    for (n = 1; n <= iterations; n++) {

        // Start Timer
        const start = process.hrtime();

        // Run WASM
        const wasm = instance._fibonacci();

        // End Timer
        const diff = process.hrtime(start);

        const result = (diff[0] * 1e9 + diff[1])/1000000000;
        results.push(result);
        console.log(wasm);
    }
    const csv = results.join('\n');

    // Write File
    fs.writeFile('results.csv', csv, function (err) {
        if (err) return console.log(err);
        console.log('Filesaved');
    });
});
\end{lstlisting}


Figure \ref{fig:AssemblyScriptResults} shows an example of how the results were stored in Excel once the experiments have been finished. 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{images/Excel.PNG}
    \caption{Gathering Results}
    \label{fig:AssemblyScriptResults}
\end{figure}

\section{Analysis of Results}

RStudio was used to perform the statistical analysis of the results. RStudio provides a convenient way of running statistical tests for normality and significance. It also provides functionality to plot graphs for visual interpretation of the results. 

Using the previously generated CSV files with the execution times of the algorithms in each language, these were imported into RStudio. The required RStudio libraries were also included. Figure \ref{fig:RStudio} shows how the libraries were included and how the CSV files were imported. The entire source code used in RStudio is included in the Appendix section of this paper.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{images/Rstudio.PNG}
    \caption{Importing Data into RStudio}
    \label{fig:RStudio}
\end{figure}

\subsection{Testing for Normal Distribution}

It is important that we understand the distribution of data in our results, as this will determine how we test for significant differences in the execution times, either with parametric tests for a normal distribution, or non-parametric tests for a non-normal distribution. 

We ran the Shapiro Wilk \cite{noauthor_shapirowilk_2021} test on each programming language result file for each algorithm. If the results from the test return a value of 0.05 or more, then it means our data has a normal distribution, otherwise it has a non-normal distribution.

In RStudio, running this test is quite a simple process. The following listing \ref{lst:RShapiro} shows and example of the R commands used to perform the Shapiro Wilk test on each algorithm for the AssemblyScript results. This process was repeated for each of the other programming languages, C, C++ and Rust.

\begin{lstlisting}[caption={Shapiro Wilk Test in RStudio},label={lst:RShapiro},language=R,basicstyle=\scriptsize,style=myStyle]
shapiro.test(AssemblyScript$BinarySearch)
shapiro.test(AssemblyScript$BubbleSort)
shapiro.test(AssemblyScript$Fibonacci)
shapiro.test(AssemblyScript$HeapSort)
shapiro.test(AssemblyScript$LinearSearch)
shapiro.test(AssemblyScript$MergeSort)
shapiro.test(AssemblyScript$NQueen24)
shapiro.test(AssemblyScript$NQueen27)
shapiro.test(AssemblyScript$PrimeNumber)
shapiro.test(AssemblyScript$SelectionSort)
shapiro.test(AssemblyScript$ShellSort)
\end{lstlisting}

The results of the Shapiro Wilk tests for normality showing the p-value confirms that our results were not normally distributed. The results can be seen in table \ref{tab:shapiro_results}. For the purpose of facilitating the visualisation of the data, we will abbreviate AssemblyScript to AS.

\begin{table}[H]
  \centering
    \begin{tabular}{| c | c | c | c | c | }
    \hline
     & \textbf{AS} & \textbf{C} & \textbf{C++} & \textbf{Rust} \\ \hline
    \textbf{BinarySearch}   & 9.081e-08     & 2.251e-09     & 4.471e-08     & 0.4335 \\ \hline
    \textbf{BubbleSort}     & 0.000131      & 2.95e-07      & 5.32e-09      & 8.002e-07 \\ \hline
    \textbf{Fibonacci}      & 1.921e-06     & 1.394e-08     & 7.82e-09      & 2.501e-07 \\ \hline
    \textbf{HeapSort}       & 2.93e-06      & 5.966e-06     & 4.656e-06     & 3.685e-07 \\ \hline
    \textbf{LinearSearch}   & 1.97e-09      & 9.529e-06     & 8.256e-05     & $<$ 2.2e-16 \\ \hline
    \textbf{MergeSort}      & 1.878e-06     & 7.426e-06     & 2.564e-09     & $<$ 2.2e-16 \\ \hline
    \textbf{NQueen24}       & 0.0005585     & 0.0002786     & $<$ 2.2e-16     & 0.01424 \\ \hline
    \textbf{NQueen27}       & 0.0009603     & 1.927e-13     & 2.8e-11       & 8.382e-12 \\ \hline
    \textbf{PrimeNumber}    & 1.986e-06     & 8.051e-09     & 1.183e-12     & 2.399e-08 \\ \hline
    \textbf{SelectionSort}  & 1.995e-13     & 2.365e-06     & 1.451e-06     & 4.591e-08 \\ \hline
    \textbf{ShellSort}      & 1.713e-07     & 9.023e-12     & 1.832e-06     & 0.04334 \\ \hline
    \end{tabular}
  \caption{Shapiro Wilk Test for Normality}
  \label{tab:shapiro_results}
\end{table}

The Shapiro Wilk test showed that our data was not normally distributed. The Central Limit Theorem \cite{noauthor_central_nodate} states that if we have sufficiently large data samples, then the distribution will be approximately normal and parametric tests can be used. However, if the sample size is not sufficiently large and the distribution of data is not normal, then non-parametric tests can be used to determine the significant difference between the samples. 

At this stage, we had to choose between running the experiments again a greater number of times until we achieved a normal distribution, so we could use parametric tests on our results, or keeping them as they are and using non-parametric tests on the results. Given the time constraints on completing this paper, we decided to use the results we had already obtained and apply non-parametric tests.

As a visual aid, we also generated some graphs to confirm the tests. We created visual graphs in both RStudio and Tableau Public \cite{noauthor_tableau_nodate}. The following figure \ref{fig:FibHist} created in Tableau Public shows the distribution of the results of the Fibonacci algorithm in the selected languages. The entire collection of graphs can be found in the Appendix of this paper.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/fibonacci_histograms.PNG}
    \caption{Fibonacci Histograms}
    \label{fig:FibHist}
\end{figure}

The following graphs created in RStudio, shown in figure \ref{fig:ASDist} represent the distribution of the results for each algorithm in AssemblyScript. The entire collection of these graphs can be found in the Appendix section of this paper.

\begin{figure}[H]
    \caption{AssemblyScript Distribtion}
    \label{fig:ASDist}
    \centering
        \subfloat[BinarySearch]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-BinarySearch.PNG}}\hfill
        \subfloat[BubbleSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-BubbleSort.PNG}}\hfill
        \subfloat[Fibonacci]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-Fibonacci.PNG}}\hfill
        \subfloat[HeapSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-HeapSort.PNG}}\hfill
        \subfloat[LinearSearch]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-LinearSearch.PNG}}\hfill
        \subfloat[MergeSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-MergeSort.PNG}}\hfill
        \subfloat[NQueen24]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-NQueen24.PNG}}\hfill
        \subfloat[NQueen27]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-NQueen27.PNG}}\hfill
        \subfloat[PrimaryNumber]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-PrimaryNumber.PNG}}\hfill
        \subfloat[SelectionSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-SelectionSort.PNG}}\hfill
        \subfloat[ShellSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-ShellSort.PNG}}\hfill
\end{figure}



\subsection{Testing for Significant Difference}

In order to accept or reject the null hypothesis, we need to determine whether there is a significant difference in the execution times of the algorithms when compared to each other. We grouped the languages into pairs and ran a non-parametric test on them to obtain the p-value. Having a p-value of 0.05 or less means that we can reject the null hypothesis as there is significant difference between the two samples.

The non-parametric test we chose is the Mann-Whitney U Test, which is used to compare two independent samples and the data is not normally distributed. We ran the test on each algorithm for AssemblyScript vs C, AssemblyScript vs C++, AssemblyScript vs Rust, C vs C++, C vs Rust and C++ vs Rust.

The following code in listing \ref{lst:MannWhitney} shows how the Mann-Whitney U Test was run on the BinarySearch algorithm for AssembyScript vs C, AssemblyScript vs C++ and AssemblyScript vs Rust. The entire RStudio code can be found in the Appendix of this paper and on our Github repository.

\begin{lstlisting}[caption={Mann-Whitney U Test in RStudio},label={lst:MannWhitney},language=R,basicstyle=\scriptsize,style=myStyle]
wilcox.test(AssemblyScript$BinarySearch,C$BinarySearch)
wilcox.test(AssemblyScript$BinarySearch,CPP$BinarySearch)
wilcox.test(AssemblyScript$BinarySearch,Rust$BinarySearch)
\end{lstlisting}

Running the Mann-Whitney U Test on each algorithm for each of our pairs of languages produced the following results. The values represented in these results are the p-value. 

In order to facilitate the visualisataion of these results in this paper, we have broken down the results into two tables. The first table \ref{tab:MannWU1} shows the results of AssemblyScript vs C, AssemblyScript vs C++ and AssemblyScript vs Rust. The second table \ref{tab:MannWU2} shows the results of C vs C++, C vs Rust and C++ vs Rust.

\begin{table}[H]
  \centering
    \begin{tabular}{| c | c | c | c |}
    \hline
     & \textbf{AS vs C} & \textbf{AS vs C++} & \textbf{AS vs Rust} \\ \hline
    \textbf{BinarySearch}   & $<$ 2.2e-16     & $<$ 2.2e-16     & $<$ 2.2e-16     \\ \hline
    \textbf{BubbleSort}     & $<$ 2.2e-16     & $<$ 2.2e-16     & $<$ 2.2e-16     \\ \hline
    \textbf{Fibonacci}      & $<$ 2.2e-16     & $<$ 2.2e-16     & $<$ 2.2e-16     \\ \hline
    \textbf{HeapSort}       & $<$ 2.2e-16     & $<$ 2.2e-16     & $<$ 2.2e-16     \\ \hline
    \textbf{LinearSearch}   & $<$ 2.2e-16     & $<$ 2.2e-16     & $<$ 2.2e-16     \\ \hline
    \textbf{MergeSort}      & $<$ 2.2e-16     & $<$ 2.2e-16     & $<$ 2.2e-16     \\ \hline
    \textbf{NQueen24}       & $<$ 2.2e-16     & $<$ 2.2e-16     & $<$ 2.2e-16     \\ \hline
    \textbf{NQueen27}       & $<$ 2.2e-16     & $<$ 2.2e-16     & $<$ 2.2e-16     \\ \hline
    \textbf{PrimeNumber}    & $<$ 2.2e-16     & $<$ 2.2e-16     & $<$ 2.2e-16     \\ \hline
    \textbf{SelectionSort}  & $<$ 2.2e-16     & $<$ 2.2e-16     & $<$ 2.2e-16     \\ \hline
    \textbf{ShellSort}      & $<$ 2.2e-16     & $<$ 2.2e-16     & 8.265e-06     \\ \hline
    \end{tabular}
  \caption{Mann-Whitney U Test}
  \label{tab:MannWU1}
\end{table}

\begin{table}[H]
  \centering
    \begin{tabular}{| c | c | c | c |}
    \hline
     & \textbf{C vs C++} & \textbf{C vs Rust} & \textbf{C++ vs Rust} \\ \hline
    \textbf{BinarySearch}   & 0.1421        & 2.427e-09     & 7.219e-12     \\ \hline
    \textbf{BubbleSort}     & 3.225e-13     & $<$ 2.2e-16   & $<$ 2.2e-16      \\ \hline
    \textbf{Fibonacci}      & 0.07379       & $<$ 2.2e-16   & 4.606e-16      \\ \hline
    \textbf{HeapSort}       & 0.6791        & 1.119e-13     & 2.901e-14     \\ \hline
    \textbf{LinearSearch}   & 0.001119      & $<$ 2.2e-16   & $<$ 2.2e-16     \\ \hline
    \textbf{MergeSort}      & 0.6215        & $<$ 2.2e-16   & $<$ 2.2e-16     \\ \hline
    \textbf{NQueen24}       & $<$ 2.2e-16   & 0.01026       & $<$ 2.2e-16    \\ \hline
    \textbf{NQueen27}       & $<$ 2.2e-16   & $<$ 2.2e-16   & $<$ 2.2e-16       \\ \hline
    \textbf{PrimeNumber}    & 2.354e-06     & $<$ 2.2e-16   & $<$ 2.2e-16     \\ \hline
    \textbf{SelectionSort}  & 0.001779      & $<$ 2.2e-16   & $<$ 2.2e-16     \\ \hline
    \textbf{ShellSort}      & 0.4074        & $<$ 2.2e-16   & $<$ 2.2e-16     \\ \hline
    \end{tabular}
  \caption{Mann-Whitney U Test}
  \label{tab:MannWU2}
\end{table}



% Results, evaluation and discussion chapter
% ============================================================= %
\chapter{Results, evaluation and discussion}

The main objective of our experiments was to answer the research question by either accepting or rejecting the null hypothesis.

\subsubsection{Research Question:}

\begin{quote}\emph{Is there a significant difference in the performance of WebAssembly modules, in terms of execution times, for the same program written in different programming languages and compiled into WebAssembly using their respective compilers?}\end{quote}

\subsubsection{H0 - Null Hpyothesis:}

\begin{quote}\emph{There is no significant difference in the performance of WebAssembly modules, in terms of execution times, for the same program written in different programming languages and compiled into WebAssembly using their respective compilers.}\end{quote}

\subsubsection{H1 - Alternate Hpyothesis:}

\begin{quote}\emph{There is a significant difference in the performance of WebAssembly modules, in terms of execution times, for the same program written in different programming languages and compiled into WebAssembly using their respective compilers.}\end{quote}

\section{Statistical Tests}

We ran statistical tests on our paired data groups to obtain the p-values, which determines whether or not there is a significant difference between grouped pairs. A p-value of 0.05 or less indicates that there is indeed a significant difference, therefore we must reject the null hypothesis and accept the alternate hypothesis. This can be summarized by the following formula:

\[ p \leq 0.05 \]

On the other hand, having a p-value of higher than 0.05 indicates that there is no significant difference between the pair, and we must accept the null hypothesis and reject the alternate hypothesis. This can be summarized by the following formula:

\[ p \textgreater 0.05 \]

The statistical tests clearly showed significant differences in almost all of the experiments. One exception was while comparing C to C++, which on some experiments had a p-value greater than 0.05. However, this was actually to be expected, since both C and C++ are compiled to WebAssembly using the same compiler and the code is almost identical. 

Figure \ref{fig:significance} highlights the results where the p-values were greater than 0.05, meaning there were no significant difference in those execution times. The results also show that for all other language pairs, there were significant differences in the execution times.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.55]{images/significance.PNG}
    \caption{P-Values}
    \label{fig:significance}
\end{figure}

\section{Visual Representations}

In order to provide a visual representation of the execution times, Tableau Public was used to produce line-charts of the execution time of each language compiled into WebAssembly for each algorithm. The visualizations clearly show that there is a significant difference in execution time between the algorithms. For visual inspection of the WebAssembly code, we used the Wabt toolkit \parencite{noauthor_webassemblywabt_2021} to transform the WASM files into a human readable format with the WAT extension.

\subsection{Numerical Computing Algorithms}

In this section we will discuss the performance of the compiled languages for each of the Numerical Computing algorithms used in our experiments. These algorithms are pure mathematical and did not require the input array of 100,000 elements.

\subsubsection{Fibonacci}

The figure \ref{fig:Fibonacci} shows the execution times for the Fibonacci algorithm. C and C++ are consistently running with similar execution times. If fact the p-values for the Fibonacci algorithm for C and C++ showed that there was no significant difference in their execution times. However, Rust appears to run faster than C and C++, while it can be visually observed that AssemblyScript performed the slowest of all the other languages.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/Fibonacci.PNG}
    \caption{Fibonacci}
    \label{fig:Fibonacci}
\end{figure}

In all of our selected programming languages, the code structure for the Fibonacci sequence have the same syntax. This can be confirmed in the following listings. As mentioned previously, we implemented the Fibonacci algorithm using the recursive method. The algorithms will produce an output with the 45th Fibonacci sequence number.

The following listing \ref{lst:FibAssembly} shows the Fibonacci algorithm in AssemblyScript.

\begin{lstlisting}[caption={Fibonacci in AssemblyScript},label={lst:FibAssembly},language=AssemblyScript,basicstyle=\scriptsize,style=myStyle]
function fib(n: i32): i32 {
  return n <= 1 ? 1 : fib(n-1) + fib(n-2);
}
export function fibonacci(): i32 {
  const n: i32 = 45;
  const result: i32 = fib(n);
  return result;
}
\end{lstlisting}

The following listing \ref{lst:FibC} shows the Fibonacci algorithm in C.

\begin{lstlisting}[caption={Fibonacci in C},label={lst:FibC},language=C,basicstyle=\scriptsize,style=myStyle]
int fib(n) {
  if (n <= 1) return 1;
  return fib(n - 1) + fib(n - 2);
}
int fibonacci() {
  int n = 45;
  int result = fib(n);
  return result;
}
\end{lstlisting}

The following listing \ref{lst:FibCPP} shows the Fibonacci algorithm in C++.

\begin{lstlisting}[caption={Fibonacci in C++},label={lst:FibCPP},language=C,basicstyle=\scriptsize,style=myStyle]
extern "C" {
  int fib(int n) {
    if (n <= 1) return 1;
    return fib(n - 1) + fib(n - 2);
  }
  int fibonacci() {
    int n = 45;
    int result = fib(n);
    return result;
  }
}
\end{lstlisting}

The following listing \ref{lst:FibRust} shows the Fibonacci algorithm in Rust. One important difference with Rust is that we need to include the a library so it can interact with JavaScript.

\begin{lstlisting}[caption={Fibonacci in Rust},label={lst:FibRust},language=Rust,basicstyle=\scriptsize,style=myStyle]
use wasm_bindgen::prelude::*;
#[wasm_bindgen]
extern "C" {
    #[wasm_bindgen(js_namespace = console)]
    fn log(s: u64);
}
fn fibonacci(n: i32) -> u64 {
    if n <= 1 { return 1 }
    return fibonacci(n-1) + fibonacci(n-2);
}
#[wasm_bindgen(start)]
pub fn main_js() {
    log(fibonacci(45));
}
\end{lstlisting}

Given that the code syntax for the Fibonacci algorithm in each of the selected languages is as consistent as possible by implementing the algorithm using the recursive method, an interesting observation is that they produced considerably different WebAssembly binaries when we performed the visual inspection of the WAT files.

The following listing \ref{lst:FibWATAS} is the WAT file representation of the compiled Fibonacci algorithm in AssemblyScript.

\begin{lstlisting}[caption={Fibonacci WAT format for AssemblyScript},label={lst:FibWATAS},language=WebAssembly,basicstyle=\scriptsize,style=myStyle]
(module
 (type $none_=>_i32 (func (result i32)))
 (type $i32_=>_i32 (func (param i32) (result i32)))
 (memory $0 0)
 (export "fibonacci" (func $assembly/index/fibonacci))
 (export "memory" (memory $0))
 (func $assembly/index/fib (param $0 i32) (result i32)
  local.get $0
  i32.const 1
  i32.le_s
  if (result i32)
   i32.const 1
  else
   local.get $0
   i32.const 1
   i32.sub
   call $assembly/index/fib
   local.get $0
   i32.const 2
   i32.sub
   call $assembly/index/fib
   i32.add
  end
 )
 (func $assembly/index/fibonacci (result i32)
  i32.const 45
  call $assembly/index/fib
 )
)
\end{lstlisting}

The following code in listing \ref{lst:FibWATC} is the WAT file representation of the compiled Fibonacci algorithm in C and C++. The WebAssembly file that was generated for C++ was identical to the one generated for C, again this was to be expected as they are both compiled using the same compiler. However, an interesting observation here is that there is considerably more code in the C and C++ version of the compiled WebAssembly when compared to AssemblyScript for the exact same algorithm. 

\begin{lstlisting}[caption={Fibonacci WAT format for C and C++},label={lst:FibWATC},language=WebAssembly,basicstyle=\scriptsize,style=myStyle]
(module
  (type (;0;) (func (result i32)))
  (type (;1;) (func (param i32) (result i32)))
  (type (;2;) (func))
  (type (;3;) (func (param i32)))
  (func (;0;) (type 2)
    nop)
  (func (;1;) (type 1) (param i32) (result i32)
    (local i32 i32)
    i32.const 1
    local.set 1
    local.get 0
    i32.const 2
    i32.ge_s
    if (result i32)  ;; label = @1
      i32.const 0
      local.set 1
      loop  ;; label = @2
        local.get 0
        i32.const 1
        i32.sub
        call 1
        local.get 1
        i32.add
        local.set 1
        local.get 0
        i32.const 3
        i32.gt_s
        local.set 2
        local.get 0
        i32.const 2
        i32.sub
        local.set 0
        local.get 2
        br_if 0 (;@2;)
      end
      local.get 1
      i32.const 1
      i32.add
    else
      local.get 1
    end)
  (func (;2;) (type 0) (result i32)
    i32.const 45
    call 1)
  (func (;3;) (type 0) (result i32)
    global.get 0)
  (func (;4;) (type 3) (param i32)
    local.get 0
    global.set 0)
  (func (;5;) (type 1) (param i32) (result i32)
    global.get 0
    local.get 0
    i32.sub
    i32.const -16
    i32.and
    local.tee 0
    global.set 0
    local.get 0)
  (func (;6;) (type 0) (result i32)
    i32.const 1024)
  (table (;0;) 1 1 funcref)
  (memory (;0;) 256 256)
  (global (;0;) (mut i32) (i32.const 5243920))
  (export "memory" (memory 0))
  (export "__wasm_call_ctors" (func 0))
  (export "fibonacci" (func 2))
  (export "__errno_location" (func 6))
  (export "stackSave" (func 3))
  (export "stackRestore" (func 4))
  (export "stackAlloc" (func 5))
  (export "__indirect_function_table" (table 0)))
\end{lstlisting}

The following listing \ref{lst:FibWATRust} is the WAT file representation of the compiled Fibonacci algorithm in Rust.

\begin{lstlisting}[caption={Fibonacci WAT format for Rust},label={lst:FibWATRust},language=WebAssembly,basicstyle=\scriptsize,style=myStyle]
(module
  (type (;0;) (func))
  (type (;1;) (func (param i32 i32)))
  (type (;2;) (func (param i32 i32 i32)))
  (type (;3;) (func (param i32) (result i32)))
  (type (;4;) (func (param i64) (result i64)))
  (import "__wbindgen_placeholder__" "__wbg_log_e2b7116aabd69db1" (func (;0;) (type 1)))
  (func (;1;) (type 4) (param i64) (result i64)
    (local i64)
    i64.const 1
    local.set 1
    local.get 0
    i64.const 2
    i64.ge_u
    if (result i64)  ;; label = @1
      i64.const 0
      local.set 1
      loop  ;; label = @2
        local.get 0
        i64.const -1
        i64.add
        call 1
        local.get 1
        i64.add
        local.set 1
        local.get 0
        i64.const -2
        i64.add
        local.tee 0
        i64.const 1
        i64.gt_u
        br_if 0 (;@2;)
      end
      local.get 1
      i64.const 1
      i64.add
    else
      local.get 1
    end)
  (func (;2;) (type 2) (param i32 i32 i32)
    (local i64)
    local.get 0
    local.get 1
    i64.extend_i32_u
    local.get 2
    i64.extend_i32_u
    i64.const 32
    i64.shl
    i64.or
    call 1
    local.tee 3
    i64.store32
    local.get 0
    local.get 3
    i64.const 32
    i64.shr_u
    i64.store32 offset=4)
  (func (;3;) (type 0)
    (local i64)
    i64.const 40
    call 1
    local.tee 0
    i32.wrap_i64
    local.get 0
    i64.const 32
    i64.shr_u
    i32.wrap_i64
    call 0)
  (func (;4;) (type 3) (param i32) (result i32)
    local.get 0
    global.get 0
    i32.add
    global.set 0
    global.get 0)
  (memory (;0;) 17)
  (global (;0;) (mut i32) (i32.const 1048576))
  (export "memory" (memory 0))
  (export "fibonacci" (func 2))
  (export "main_js" (func 3))
  (export "__wbindgen_add_to_stack_pointer" (func 4))
  (export "__wbindgen_start" (func 3)))
\end{lstlisting}

Based on our experiment and analysis of the Fibonacci algorithm, we can clearly see a significant difference in the performance of the WebAssembly modules from different programming languages, with the exception of C and C++ that produced identical WebAssembly files and had a p-value greater than 0.05. We can also see a clear difference of the WebAssembly files when converted into human readable format.

In order to not unnecessarily bloat the contents of this paper, the source code and WAT files for the remaining algorithms will not be included directly in the paper. However, they may be viewed on our Github repository.

\subsubsection{PrimeNumber}

The visualization in figure \ref{fig:Prime} shows that C and C++ perform with similar execution times, although their p-values were less than 0.05. We can also see that Rust performs slightly slower than C and C++. Both however generally performed the PrimeNumber algorithm in less than 1 second every time. On the other hand, we can clearly see that AssemblyScript took longer than all the other languages, taking between 4 to 5 seconds to complete.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/PrimeNumber.PNG}
    \caption{PrimeNumber}
    \label{fig:Prime}
\end{figure}

We confirmed that the syntax for the PrimaryNumber algorithm is the same for each of the selected languages. However, the WebAssembly files generated were considerably different. We observed that the AssemblyScript produced 279 lines of code when the WAT extension file was inspected, the C and C++ versions produced only 36 lines of code, while the Rust version produced 7005 lines of code. 

\subsubsection{NQueen24 and NQueen27}

The source code for the NQueens algorithms for each of the selected languages were kept syntactically the same as much as possible in our experiments. Upon inspection of the WAT files for each of the selected languages, we observed that AssemblyScript generated 3090 lines, C and C++ generated 3218 lines, while Rust generated 7622 lines.

The execution times of the NQueen24 and NQueen27 algorithms can be viewed in figure \ref{fig:NQ24} and figure \ref{fig:NQ27}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/NQueen24.PNG}
    \caption{NQueen24}
    \label{fig:NQ24}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/NQueen27.PNG}
    \caption{NQueen27}
    \label{fig:NQ27}
\end{figure}

From these visualizations we can see that C, C++ and Rust performed with similar execution times, generally with less than 1 second every time. However, there was a clear difference in the execution times for AssemblyScript, which took between 3 and 5 seconds per execution.


\subsection{Searching Algorithms}

The source code for the Searching algorithms were kept syntactically the same as much as possible. An array of 100,000 elements was hard-coded into each of the algorithms. This caused the WebAssembly compilers generated huge WASM and WAT files for each language.

\subsubsection{BinarySearch}

For the BinarySearch algorithm, all languages had visually similar execution times, but it is still clear that there are differences between their performances. The statistical tests for C and C++ returned a value greater than 0.05, meaning that there was no significant difference between their execution times. However, Rust appears to have the fastest execution times over C and C++, while AssemblyScript has once again the slowest execution times. There were also significant differences in the inspection of the WAT files for the BinarySearch algorithm when converted into WebAssembly. The execution times for the BinarySearch algorithm can be view in figure \ref{fig:BS}. 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/BinarySearch.PNG}
    \caption{BinarySearch}
    \label{fig:BS}
\end{figure}

\subsubsection{LinearSearch}

The LinearSearch algoritm shows that all languages had significant differences in their execution times, although visually, C and C++ appear to have similar performances. In this case, Rust had the fastest execution times, generally running at around 4 seconds each time. C and C++ executed at about 5 to 6 seconds each time, while AssemblyScript had the slowest execution times, performing between 12 and 16 seconds for each iteration. The figure \ref{fig:LS} shows the execution times for the LinearSearch algorithm in the selected languages.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/LinearSearch.PNG}
    \caption{LinearSearch}
    \label{fig:LS}
\end{figure}

\subsection{Sorting Algorithms}

From the following visualizations, it can be determined that C and C++ consistently had the fastest execution times over the other languages, while Rust and AssemblyScript had a mixed result, with sometimes one performing better than the other.

\subsubsection{HeapSort}

According to the statistical tests we ran on the HeapSort algorithm, C and C++ had a p-value of greater than 0.05, meaning that there was no significant difference between their execution times. This can be visually verified in figure \ref{fig:Heap}. An interesting observation is that Rust appears to start slowly and then speed up during the experiment. This experiment was repeated many times and the same behavior from Rust was always observed. The results show that C and C++ had the fastest times, followed by Rust, and AssemblyScript had the slowest times.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/HeapSort.PNG}
    \caption{HeapSort}
    \label{fig:Heap}
\end{figure}


\subsubsection{MergeSort}

The statistical tests performed on the MergeSort algorithm also returned a p-value greater than 0.05 for C and C++, meaning there was no significant difference between their execution times. This is visible on figure \ref{fig:Merge}. Here we can see that once again Rust starts off slower than begins to speed up. Another interesting observation is that AssemblyScript actually runs faster than Rust for this algorithm, while C and C++ have once again the fastest execution times.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/MergeSort.PNG}
    \caption{MergeSort}
    \label{fig:Merge}
\end{figure}

\subsubsection{ShellSort}

The p-values for the ShellSort algorithm for C and C++ again came back with values greater than 0.05, meaning that there was no significant difference between their execution times. This can be visually confirmed on figure \ref{fig:Shell}. It can also be observed that Rust once again starts off slower, then speeds up towards the end of the execution. C and C++ have the fastest execution times, while Rust and AssemblyScript have visually similar execution times.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/ShellSort.PNG}
    \caption{ShellSort}
    \label{fig:Shell}
\end{figure}

\subsubsection{SelectionSort}

The SelectionSort visualization shows that C and C++ have visually similar performances, however the statistical tests returned a p-value of less than 0.05. From this visualization we can see that C and C++ generally took around 4 seconds to execute, while Rust took around 8 seconds. The performance of AssemblyScript however was much slower, taking between 30 and 40 seconds for each iteration.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/SelectionSort.PNG}
    \caption{SelectionSort}
    \label{fig:Selection}
\end{figure}

\subsubsection{BubbleSort}

The BubbleSort algorithm revealed great differences between the performance of AssemblyScript and the other languages. While C and C++ performed slightly faster than Rust, with execution times of around 7 to 8 seconds, and Rust with execution times of around 9 to 10 seconds, AssemblyScript had taken between 70 and 80 seconds to perform each iteration.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/BubbleSort.PNG}
    \caption{BubbleSort}
    \label{fig:Bubble}
\end{figure}

\section{Discussion}

WebAssembly itself is generally still in its infancy stages, and many features are currently still being developed. This imposes some restrictions on what compilers can actually compile into WebAssembly at this time. Overall, the performance of AssemblyScript was considerably slower than Rust, C and C++.

The performance times of C and C++ were consistently faster than all of the other languages. Even though the statistical tests proved that sometimes there were no significant difference between C and C++, we believe that given a much higher number of iterations in each experiment, that the statistical tests would eventually show no significant difference between all of the algorithms implemented in C and C++.

Although not within the scope of this research, there was another interesting observation made regarding the compilers. The C, C++ and AssemblyScript modules usually compiled with reasonable speeds, normally ranging between 1 and 5 seconds for each algorithm. However, for the algorithms with the arrays of 100,000 elements, the Rust compiler would take around 30 minutes to compile.

The research work done in this paper highlights an important aspect of WebAssembly compilers, in that they do in fact suffer from significantly different performance times, even if the source code is syntactically identical in the various different programming languages. The idea behind WebAssembly is to allow multiple languages, such as C and Rust, to be used on the web. However, since the compilers will ultimately produce different WebAssembly files with varying performances, developers should give careful consideration into which language and compiler they use to generate WebAssembly modules.

Given more time to complete this research paper, the number of iterations for each algorithm would ideally be increased in order to produce a normal distribution of data and to use parametric tests to either accept of reject the hypothesis. Another aspect that could be improved is the choice of algorithms for each language. We believe that the Ostrich Benchmark Suite would be a suitable candidate. Ideally these benchmarks would be written in a large variety of programming languages, such as Haskell and Prolog, covering a variety of programming paradigms such as procedural, object-oriented, function and logical. We believe the analysis of the same algorithms written across a large spectrum of programming languages and paradigms converted into WebAssembly would provide interesting and useful insights.

One final observation that we will make in our evaluation is the comparison of the total time in milliseconds that each language took to execute each of the algorithms for every iteration. This resulted in C++ being the fastest overall, followed C and Rust, with AssemblyScript being the slowest overall. This reveals that currently, although WebAssembly promises speeds of near native performance otherwise only achieved with C/C++, that WebAssembly compiled from C/C++ still provide the best performances over other languages compiled into WebAssembly. This can be viewed in figure \ref{fig:Totals}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{images/Totals.PNG}
    \caption{Overall Performance in milliseconds}
    \label{fig:Totals}
\end{figure}

% Concluding chapter
% ============================================================= %
\chapter{Conclusion}
\section{Research Overview}

WebAssembly is becoming more and more popular in research and web-development as it promises improved performance to compute-intensive operations for web-based applications. In this paper we critically evaluate the current literature that benchmarks WebAssembly performance compared with traditional implementations. In our opinion, there is no doubt that WebAssembly offers an exciting and efficient alternative to many of the limitations found in web-based and cloud-based applications of today. 

However, we raise the concern that the performance of WebAssembly is greatly impacted by the choice of compiler used, meaning that certain programming languages will still have significant differences in their performance when compiled to WebAssembly. At the time of writing this paper, we did not find any other research paper that compares the code generated by WebAssembly compilers and evaluates their performance against each other.

\section{Problem Definition}

The objective of this research was to evaluate the performance of WebAssembly code generated by different compilers, highlighting a potential issue with the performance of the generated code. For example, if we write even a simple loop in C, C++, Rust and AssemblyScript, one might initially assume that the WebAssembly code produced by the compilers would be similar and would run without significant differences in performance. However, we have highlighted that this is not the case. As programs become more complex, the differences in performance of the generated WebAssembly code may become significant in terms of execution speed and file size.

\section{Design/Experimentation, Evaluation \& Results}

For our experiments, we used a selection of 11 different programs written in C, C++, Rust and AssemblyScript. These programs ranged from simple Fibonacci, Prime Number, Searching and Sorting functions to more demanding numerical computing functions such as the N-Queens problem. We compiled each program into WebAssembly using the respective compiler for each programming language.

We used NodeJS to run the WebAssembly compiled functions and measure their execution times. Each function was run a total of 120 times. Upon inspection of the data produced by these experiments, we determined that the data did not have a normal distribution. This in turn influenced the choice of statistical test that we had to use to determine whether or not there were significant differences in the performance of the compiled WebAssembly modules.

Our results showed that in fact there are significant differences in the performance of WebAssembly modules generated by the different compilers, not only in terms of execution times, but upon visual inspection of the human-readable format of the WebAssembly code, we can clearly see significant differences even for simple programs compiled to WebAssembly. 

We also revealed that, although WebAssembly promises performance of near native speed otherwise only achieved with C/C++, currently WebAssembly compiled from C/C++ still offer the best performance over the other programming languages used in our research.


\section{Contributions and impact}

In this research we have identified a potential problem with WebAssembly compilers, in that they produce vastly different byte code for even simple programs, and significant differences in their performance, depending on which compiler was used to generate the WebAssembly module. As we are relatively still in the early days of WebAssembly, identifying these issues should highlight the need for some standardization in the WebAssembly code produced by compilers. We believe that this will provide valuable insights for developers as they choose which programming language to develop in and subsequently compile into WebAssembly. 

We also provided a critical evaluation of the literature review currently available for WebAssembly benchmarking and for WebAssembly solutions being proposed as alternatives to existing approaches in applications. We highlighted that for evaluating the performance of WebAssembly, using only one compiler is not sufficient, as different WebAssembly compilers may produce different results.


\section{Future Work \& recommendations}

In chapter 1 of this paper we identified the scope and limitations of our research. These limitations create opportunities for future work in this area. We recommend that more research be done on the comparison of code generated by WebAssembly compilers to identify the differences between their produced code and determine the reasons why these differences exist. Another area of interest could be to compare the file size of the compiled WebAssembly modules from different compilers while compiling the same programming logic.

The list of programming languages that currently compile to WebAssembly can be found at \parencite{appcypher_appcypherawesome-wasm-langs_2021}. Future research could be done to evaluate the performance of these compilers. Furthermore, as more and more WebAssembly compilers emerge, programming languages with different programming paradigms will be able to compile to WebAssembly. For example, Haskell is a functional programming language, while ProLog is a logic programming language.  We believe that analysing the WebAssembly byte code and performance of functional, logical, procedural and object-oriented programming languages will be of interest to researchers and application developers.

The Ostrich Benchmark Suite was published in \parencite{Khan2015}. They identified important patterns for numerical computation and ran experiments to compare JavaScript performance against native C code for sequential and parallel operations. Therefore another possibility for future work would be to implement the numerical computing benchmarks identified in the Ostrich Benchmark Suite in multiple programming languages, including languages from different programming paradigms such as procedural, functional, logical and object-oriented, and evaluating the performance of these benchmarks in WebAssembly from multiple compilers.

Finally, since our experiments were all executed on only a NodeJS environment, future work might also involve running such experiments on all modern web-browsers and various IoT devices.

% End of thesis content
% ============================================================= %

% Include the bibligraphy by referencing the correct .bib file
\printbibliography

% Optional appendices
\appendix

\chapter{Source Code}

The entire code and results for these experiments can be found at:

https://github.com/rayphelan/MastersProject2021

\hfill \break

Listing \ref{lst:FullRStudio} contains the entire RStudio code used for performing statistical tests.

\begin{lstlisting}[caption={Statistical Tests in RStudio},label={lst:FullRStudio},language=R,basicstyle=\scriptsize,style=myStyle]
library("dplyr")
library("ggpubr")

##############################################################################
# Load Languages
AssemblyScript <- read.csv('C:/TuDublin/Masters Project/Git Clone/MastersProject2021/
FinalResultsPerLanguage/CSV/AssemblyScript.csv')

C <- read.csv('C:/TuDublin/Masters Project/Git Clone/MastersProject2021/
FinalResultsPerLanguage/CSV/C.csv')

CPP <- read.csv('C:/TuDublin/Masters Project/Git Clone/MastersProject2021/
FinalResultsPerLanguage/CSV/CPP.csv')

Rust <- read.csv('C:/TuDublin/Masters Project/Git Clone/MastersProject2021/
FinalResultsPerLanguage/CSV/Rust.csv')


##############################################################################
# Density Plot - AssemblyScript 
ggdensity(AssemblyScript$BinarySearch)
ggdensity(AssemblyScript$BubbleSort)
ggdensity(AssemblyScript$Fibonacci)
ggdensity(AssemblyScript$HeapSort)
ggdensity(AssemblyScript$LinearSearch)
ggdensity(AssemblyScript$MergeSort)
ggdensity(AssemblyScript$NQueen24)
ggdensity(AssemblyScript$NQueen27)
ggdensity(AssemblyScript$PrimeNumber)
ggdensity(AssemblyScript$SelectionSort)
ggdensity(AssemblyScript$ShellSort)

# Normality Plot - AssemblyScript
ggqqplot(AssemblyScript$BinarySearch)
ggqqplot(AssemblyScript$BubbleSort)
ggqqplot(AssemblyScript$Fibonacci)
ggqqplot(AssemblyScript$HeapSort)
ggqqplot(AssemblyScript$LinearSearch)
ggqqplot(AssemblyScript$MergeSort)
ggqqplot(AssemblyScript$NQueen24)
ggqqplot(AssemblyScript$NQueen27)
ggqqplot(AssemblyScript$PrimeNumber)
ggqqplot(AssemblyScript$SelectionSort)
ggqqplot(AssemblyScript$ShellSort)

# Histograms - AssemblyScript
hist(AssemblyScript$BinarySearch)
hist(AssemblyScript$BubbleSort)
hist(AssemblyScript$Fibonacci)
hist(AssemblyScript$HeapSort)
hist(AssemblyScript$LinearSearch)
hist(AssemblyScript$MergeSort)
hist(AssemblyScript$NQueen24)
hist(AssemblyScript$NQueen27)
hist(AssemblyScript$PrimeNumber)
hist(AssemblyScript$SelectionSort)
hist(AssemblyScript$ShellSort)

# Shapiro-Wilk normality test - AssemblyScript
shapiro.test(AssemblyScript$BinarySearch)     # W = 0.89333,  p-value = 9.081e-08
shapiro.test(AssemblyScript$BubbleSort)       # W = 0.947,    p-value = 0.000131  
shapiro.test(AssemblyScript$Fibonacci)        # W = 0.9183,   p-value = 1.921e-06
shapiro.test(AssemblyScript$HeapSort)         # W = 0.92146,  p-value = 2.93e-06
shapiro.test(AssemblyScript$LinearSearch)     # W = 0.85618,  p-value = 1.97e-09
shapiro.test(AssemblyScript$MergeSort)        # W = 0.91813,  p-value = 1.878e-06 
shapiro.test(AssemblyScript$NQueen24)         # W = 0.95549,  p-value = 0.0005585 
shapiro.test(AssemblyScript$NQueen27)         # W = 0.9585,   p-value = 0.0009603 
shapiro.test(AssemblyScript$PrimeNumber)      # W = 0.91855,  p-value = 1.986e-06
shapiro.test(AssemblyScript$SelectionSort)    # W = 0.73501,  p-value = 1.995e-13
shapiro.test(AssemblyScript$ShellSort)        # W = 0.89884,  p-value = 1.713e-07

# independent 2-group Mann-Whitney U Tests

# BinarySearch
wilcox.test(AssemblyScript$BinarySearch,C$BinarySearch)     # W = 12582,  p-value < 2.2e-16
wilcox.test(AssemblyScript$BinarySearch,CPP$BinarySearch)   # W = 12408,  p-value < 2.2e-16
wilcox.test(AssemblyScript$BinarySearch,Rust$BinarySearch)  # W = 13720,  p-value < 2.2e-16

# BubbleSort
wilcox.test(AssemblyScript$BubbleSort,C$BubbleSort)         # W = 14400,  p-value < 2.2e-16
wilcox.test(AssemblyScript$BubbleSort,CPP$BubbleSort)       # W = 14400,  p-value < 2.2e-16
wilcox.test(AssemblyScript$BubbleSort,Rust$BubbleSort)      # W = 14400,  p-value < 2.2e-16

# Fibonacci
wilcox.test(AssemblyScript$Fibonacci,C$Fibonacci)           # W = 14400,  p-value < 2.2e-16
wilcox.test(AssemblyScript$Fibonacci,CPP$Fibonacci)         # W = 14400,  p-value < 2.2e-16
wilcox.test(AssemblyScript$Fibonacci,Rust$Fibonacci)        # W = 14400,  p-value < 2.2e-16

# HeapSort
wilcox.test(AssemblyScript$HeapSort,C$HeapSort)             # W = 14400,  p-value < 2.2e-16
wilcox.test(AssemblyScript$HeapSort,CPP$HeapSort)           # W = 14400,  p-value < 2.2e-16
wilcox.test(AssemblyScript$HeapSort,Rust$HeapSort)          # W = 14400,  p-value < 2.2e-16

# LinearSearch
wilcox.test(AssemblyScript$LinearSearch,C$LinearSearch)     # W = 14400,  p-value < 2.2e-16
wilcox.test(AssemblyScript$LinearSearch,CPP$LinearSearch)   # W = 14400,  p-value < 2.2e-16
wilcox.test(AssemblyScript$LinearSearch,Rust$LinearSearch)  # W = 14400,  p-value < 2.2e-16

# MergeSort
wilcox.test(AssemblyScript$MergeSort,C$MergeSort)           # W = 14400,  p-value < 2.2e-16
wilcox.test(AssemblyScript$MergeSort,CPP$MergeSort)         # W = 14400,  p-value < 2.2e-16
wilcox.test(AssemblyScript$MergeSort,Rust$MergeSort)        # W = 362,    p-value < 2.2e-16

# NQueen24
wilcox.test(AssemblyScript$NQueen24,C$NQueen24)             # W = 14400,  p-value < 2.2e-16
wilcox.test(AssemblyScript$NQueen24,CPP$NQueen24)           # W = 14400,  p-value < 2.2e-16
wilcox.test(AssemblyScript$NQueen24,Rust$NQueen24)          # W = 14400,  p-value < 2.2e-16

# NQueen27
wilcox.test(AssemblyScript$NQueen27,C$NQueen27)             # W = 14400,  p-value < 2.2e-16
wilcox.test(AssemblyScript$NQueen27,CPP$NQueen27)           # W = 14400,  p-value < 2.2e-16
wilcox.test(AssemblyScript$NQueen27,Rust$NQueen27)          # W = 14400,  p-value < 2.2e-16

# PrimeNumber
wilcox.test(AssemblyScript$PrimeNumber,C$PrimeNumber)       # W = 14400,  p-value < 2.2e-16
wilcox.test(AssemblyScript$PrimeNumber,CPP$PrimeNumber)     # W = 14400,  p-value < 2.2e-16
wilcox.test(AssemblyScript$PrimeNumber,Rust$PrimeNumber)    # W = 14400,  p-value < 2.2e-16

# SelectionSort
wilcox.test(AssemblyScript$SelectionSort,C$SelectionSort)     # W = 14400, p-value < 2.2e-16
wilcox.test(AssemblyScript$SelectionSort,CPP$SelectionSort)   # W = 14400, p-value < 2.2e-16
wilcox.test(AssemblyScript$SelectionSort,Rust$SelectionSort)  # W = 14400, p-value < 2.2e-16

# ShellSort
wilcox.test(AssemblyScript$ShellSort,C$ShellSort)           # W = 14325,  p-value < 2.2e-16
wilcox.test(AssemblyScript$ShellSort,CPP$ShellSort)         # W = 14400,  p-value < 2.2e-16
wilcox.test(AssemblyScript$ShellSort,Rust$ShellSort)        # W = 9598,   p-value = 8.265e-06


##############################################################################
# Density Plot - C 
ggdensity(C$BinarySearch)
ggdensity(C$BubbleSort)
ggdensity(C$Fibonacci)
ggdensity(C$HeapSort)
ggdensity(C$LinearSearch)
ggdensity(C$MergeSort)
ggdensity(C$NQueen24)
ggdensity(C$NQueen27)
ggdensity(C$PrimeNumber)
ggdensity(C$SelectionSort)
ggdensity(C$ShellSort)

# Normality Plot 1 - C
ggqqplot(C$BinarySearch)
ggqqplot(C$BubbleSort)
ggqqplot(C$Fibonacci)
ggqqplot(C$HeapSort)
ggqqplot(C$LinearSearch)
ggqqplot(C$MergeSort)
ggqqplot(C$NQueen24)
ggqqplot(C$NQueen27)
ggqqplot(C$PrimeNumber)
ggqqplot(C$SelectionSort)
ggqqplot(C$ShellSort)

# Histograms - C
hist(C$BinarySearch)
hist(C$BubbleSort)
hist(C$Fibonacci)
hist(C$HeapSort)
hist(C$LinearSearch)
hist(C$MergeSort)
hist(C$NQueen24)
hist(C$NQueen27)
hist(C$PrimeNumber)
hist(C$SelectionSort)
hist(C$ShellSort)

# Shapiro-Wilk normality test - C
shapiro.test(C$BinarySearch)    # W = 0.85759,  p-value = 2.251e-09
shapiro.test(C$BubbleSort)      # W = 0.90343,  p-value = 2.95e-07
shapiro.test(C$Fibonacci)       # W = 0.87602,  p-value = 1.394e-08
shapiro.test(C$HeapSort)        # W = 0.92663,  p-value = 5.966e-06
shapiro.test(C$LinearSearch)    # W = 0.92993,  p-value = 9.529e-06
shapiro.test(C$MergeSort)       # W = 0.92818,  p-value = 7.426e-06
shapiro.test(C$NQueen24)        # W = 0.95149,  p-value = 0.0002786
shapiro.test(C$NQueen27)        # W = 0.73445,  p-value = 1.927e-13
shapiro.test(C$PrimeNumber)     # W = 0.87064,  p-value = 8.051e-09
shapiro.test(C$SelectionSort)   # W = 0.91986,  p-value = 2.365e-06
shapiro.test(C$ShellSort)       # W = 0.79129,  p-value = 9.023e-12


# independent 2-group Mann-Whitney U Tests

# BinarySearch
wilcox.test(C$BinarySearch,AssemblyScript$BinarySearch)     # W = 1818,   p-value < 2.2e-16
wilcox.test(C$BinarySearch,CPP$BinarySearch)                # W = 6410,   p-value = 0.1421 
wilcox.test(C$BinarySearch,Rust$BinarySearch)               # W = 10409,  p-value = 2.427e-09

# BubbleSort
wilcox.test(C$BubbleSort,AssemblyScript$BubbleSort)         # W = 0,      p-value < 2.2e-16
wilcox.test(C$BubbleSort,CPP$BubbleSort)                    # W = 11118,  p-value = 3.225e-13
wilcox.test(C$BubbleSort,Rust$BubbleSort)                   # W = 626,    p-value < 2.2e-16

# Fibonacci
wilcox.test(C$Fibonacci,AssemblyScript$Fibonacci)           # W = 0,      p-value < 2.2e-16
wilcox.test(C$Fibonacci,CPP$Fibonacci)                      # W = 8162,   p-value = 0.07379 
wilcox.test(C$Fibonacci,Rust$Fibonacci)                     # W = 12296,  p-value < 2.2e-16

# HeapSort
wilcox.test(C$HeapSort,AssemblyScript$HeapSort)             # W = 0,      p-value < 2.2e-16
wilcox.test(C$HeapSort,CPP$HeapSort)                        # W = 7423,   p-value = 0.6791  
wilcox.test(C$HeapSort,Rust$HeapSort)                       # W = 3206,   p-value = 1.119e-13

# LinearSearch
wilcox.test(C$LinearSearch,AssemblyScript$LinearSearch)     # W = 0,      p-value < 2.2e-16
wilcox.test(C$LinearSearch,CPP$LinearSearch)                # W = 8953,   p-value = 0.001119
wilcox.test(C$LinearSearch,Rust$LinearSearch)               # W = 14280,  p-value < 2.2e-16

# MergeSort
wilcox.test(C$MergeSort,AssemblyScript$MergeSort)           # W = 0,      p-value < 2.2e-16
wilcox.test(C$MergeSort,CPP$MergeSort)                      # W = 6934,   p-value = 0.6215
wilcox.test(C$MergeSort,Rust$MergeSort)                     # W = 0,      p-value < 2.2e-16

# NQueen24
wilcox.test(C$NQueen24,AssemblyScript$NQueen24)             # W = 0,      p-value < 2.2e-16
wilcox.test(C$NQueen24,CPP$NQueen24)                        # W = 14090,  p-value < 2.2e-16
wilcox.test(C$NQueen24,Rust$NQueen24)                       # W = 5819,   p-value = 0.01026

# NQueen27
wilcox.test(C$NQueen27,AssemblyScript$NQueen27)             # W = 0,      p-value < 2.2e-16
wilcox.test(C$NQueen27,CPP$NQueen27)                        # W = 12789,  p-value < 2.2e-16
wilcox.test(C$NQueen27,Rust$NQueen27)                       # W = 2604,   p-value < 2.2e-16

# PrimeNumber
wilcox.test(C$PrimeNumber,AssemblyScript$PrimeNumber)       # W = 0,      p-value < 2.2e-16
wilcox.test(C$PrimeNumber,CPP$PrimeNumber)                  # W = 4661,   p-value = 2.354e-06
wilcox.test(C$PrimeNumber,Rust$PrimeNumber)                 # W = 0,      p-value < 2.2e-16

# SelectionSort
wilcox.test(C$SelectionSort,AssemblyScript$SelectionSort)   # W = 0,      p-value < 2.2e-16
wilcox.test(C$SelectionSort,CPP$SelectionSort)              # W = 8881,   p-value = 0.001779
wilcox.test(C$SelectionSort,Rust$SelectionSort)             # W = 0,      p-value < 2.2e-16

# ShellSort
wilcox.test(C$ShellSort,AssemblyScript$ShellSort)           # W = 75,     p-value < 2.2e-16
wilcox.test(C$ShellSort,CPP$ShellSort)                      # W = 6754,   p-value = 0.4074
wilcox.test(C$ShellSort,Rust$ShellSort)                     # W = 529,    p-value < 2.2e-16


##############################################################################
# Density Plot - CPP 
ggdensity(CPP$BinarySearch)
ggdensity(CPP$BubbleSort)
ggdensity(CPP$Fibonacci)
ggdensity(CPP$HeapSort)
ggdensity(CPP$LinearSearch)
ggdensity(CPP$MergeSort)
ggdensity(CPP$NQueen24)
ggdensity(CPP$NQueen27)
ggdensity(CPP$PrimeNumber)
ggdensity(CPP$SelectionSort)
ggdensity(CPP$ShellSort)

# Normality Plot 1 - CPP
ggqqplot(CPP$BinarySearch)
ggqqplot(CPP$BubbleSort)
ggqqplot(CPP$Fibonacci)
ggqqplot(CPP$HeapSort)
ggqqplot(CPP$LinearSearch)
ggqqplot(CPP$MergeSort)
ggqqplot(CPP$NQueen24)
ggqqplot(CPP$NQueen27)
ggqqplot(CPP$PrimeNumber)
ggqqplot(CPP$SelectionSort)
ggqqplot(CPP$ShellSort)

# Histograms - CPP
hist(CPP$BinarySearch)
hist(CPP$BubbleSort)
hist(CPP$Fibonacci)
hist(CPP$HeapSort)
hist(CPP$LinearSearch)
hist(CPP$MergeSort)
hist(CPP$NQueen24)
hist(CPP$NQueen27)
hist(CPP$PrimeNumber)
hist(CPP$SelectionSort)
hist(CPP$ShellSort)

# Shapiro-Wilk normality test - CPP
shapiro.test(CPP$BinarySearch)    # W = 0.88697,  p-value = 4.471e-08
shapiro.test(CPP$BubbleSort)      # W = 0.86648,  p-value = 5.32e-09
shapiro.test(CPP$Fibonacci)       # W = 0.87035,  p-value = 7.82e-09
shapiro.test(CPP$HeapSort)        # W = 0.92485,  p-value = 4.656e-06
shapiro.test(CPP$LinearSearch)    # W = 0.94416,  p-value = 8.256e-05
shapiro.test(CPP$MergeSort)       # W = 0.85896,  p-value = 2.564e-09
shapiro.test(CPP$NQueen24)        # W = 0.56648,  p-value < 2.2e-16
shapiro.test(CPP$NQueen27)        # W = 0.80628,  p-value = 2.8e-11
shapiro.test(CPP$PrimeNumber)     # W = 0.76245,  p-value = 1.183e-12
shapiro.test(CPP$SelectionSort)   # W = 0.91616,  p-value = 1.451e-06
shapiro.test(CPP$ShellSort)       # W = 0.91794,  p-value = 1.832e-06

# independent 2-group Mann-Whitney U Tests

# BinarySearch
wilcox.test(CPP$BinarySearch,AssemblyScript$BinarySearch)    # W = 1992,   p-value < 2.2e-16
wilcox.test(CPP$BinarySearch,C$BinarySearch)                 # W = 7990,   p-value = 0.1421
wilcox.test(CPP$BinarySearch,Rust$BinarySearch)              # W = 10886,  p-value = 7.219e-12

# BubbleSort
wilcox.test(CPP$BubbleSort,AssemblyScript$BubbleSort)        # W = 0,      p-value < 2.2e-16
wilcox.test(CPP$BubbleSort,C$BubbleSort)                     # W = 3282,   p-value = 3.225e-13
wilcox.test(CPP$BubbleSort,Rust$BubbleSort)                  # W = 73,     p-value < 2.2e-16

# Fibonacci
wilcox.test(CPP$Fibonacci,AssemblyScript$Fibonacci)          # W = 0,      p-value < 2.2e-16
wilcox.test(CPP$Fibonacci,C$Fibonacci)                       # W = 6238,   p-value = 0.07379
wilcox.test(CPP$Fibonacci,Rust$Fibonacci)                    # W = 11568,  p-value = 4.606e-16

# HeapSort
wilcox.test(CPP$HeapSort,AssemblyScript$HeapSort)            # W = 0,      p-value < 2.2e-16
wilcox.test(CPP$HeapSort,C$HeapSort)                         # W = 6977,   p-value = 0.6791
wilcox.test(CPP$HeapSort,Rust$HeapSort)                      # W = 3111,   p-value = 2.901e-14

# LinearSearch
wilcox.test(CPP$LinearSearch,AssemblyScript$LinearSearch)    # W = 0,      p-value < 2.2e-16
wilcox.test(CPP$LinearSearch,C$LinearSearch)                 # W = 5447,   p-value = 0.001119
wilcox.test(CPP$LinearSearch,Rust$LinearSearch)              # W = 14280,  p-value < 2.2e-16

# MergeSort
wilcox.test(CPP$MergeSort,AssemblyScript$MergeSort)          # W = 0,      p-value < 2.2e-16
wilcox.test(CPP$MergeSort,C$MergeSort)                       # W = 7466,   p-value = 0.6215
wilcox.test(CPP$MergeSort,Rust$MergeSort)                    # W = 0,      p-value < 2.2e-16

# NQueen24
wilcox.test(CPP$NQueen24,AssemblyScript$NQueen24)            # W = 0,      p-value < 2.2e-16
wilcox.test(CPP$NQueen24,C$NQueen24)                         # W = 310,    p-value < 2.2e-16
wilcox.test(CPP$NQueen24,Rust$NQueen24)                      # W = 781,    p-value < 2.2e-16

# NQueen27
wilcox.test(CPP$NQueen27,AssemblyScript$NQueen27)            # W = 0,      p-value < 2.2e-16
wilcox.test(CPP$NQueen27,C$NQueen27)                         # W = 1611,   p-value < 2.2e-16
wilcox.test(CPP$NQueen27,Rust$NQueen27)                      # W = 872,    p-value < 2.2e-16

# PrimeNumber
wilcox.test(CPP$PrimeNumber,AssemblyScript$PrimeNumber)      # W = 0,      p-value < 2.2e-16
wilcox.test(CPP$PrimeNumber,C$PrimeNumber)                   # W = 9739,   p-value = 2.354e-06
wilcox.test(CPP$PrimeNumber,Rust$PrimeNumber)                # W = 0,      p-value < 2.2e-16

# SelectionSort
wilcox.test(CPP$SelectionSort,AssemblyScript$SelectionSort)  # W = 0,      p-value < 2.2e-16
wilcox.test(CPP$SelectionSort,C$SelectionSort)               # W = 5519,   p-value = 0.001779
wilcox.test(CPP$SelectionSort,Rust$SelectionSort)            # W = 0,      p-value < 2.2e-16

# ShellSort
wilcox.test(CPP$ShellSort,AssemblyScript$ShellSort)          # W = 0,      p-value < 2.2e-16
wilcox.test(CPP$ShellSort,C$ShellSort)                       # W = 7646,   p-value = 0.4074
wilcox.test(CPP$ShellSort,Rust$ShellSort)                    # W = 565,    p-value < 2.2e-16


##############################################################################
# Density Plot - Rust 
ggdensity(Rust$BinarySearch)
ggdensity(Rust$BubbleSort)
ggdensity(Rust$Fibonacci)
ggdensity(Rust$HeapSort)
ggdensity(Rust$LinearSearch)
ggdensity(Rust$MergeSort)
ggdensity(Rust$NQueen24)
ggdensity(Rust$NQueen27)
ggdensity(Rust$PrimeNumber)
ggdensity(Rust$SelectionSort)
ggdensity(Rust$ShellSort)

# Normality Plot 1 - Rust
ggqqplot(Rust$BinarySearch)
ggqqplot(Rust$BubbleSort)
ggqqplot(Rust$Fibonacci)
ggqqplot(Rust$HeapSort)
ggqqplot(Rust$LinearSearch)
ggqqplot(Rust$MergeSort)
ggqqplot(Rust$NQueen24)
ggqqplot(Rust$NQueen27)
ggqqplot(Rust$PrimeNumber)
ggqqplot(Rust$SelectionSort)
ggqqplot(Rust$ShellSort)

# Histograms - Rust
hist(Rust$BinarySearch)
hist(Rust$BubbleSort)
hist(Rust$Fibonacci)
hist(Rust$HeapSort)
hist(Rust$LinearSearch)
hist(Rust$MergeSort)
hist(Rust$NQueen24)
hist(Rust$NQueen27)
hist(Rust$PrimeNumber)
hist(Rust$SelectionSort)
hist(Rust$ShellSort)

# Shapiro-Wilk normality test - Rust
shapiro.test(Rust$BinarySearch)     # W = 0.9888,   p-value = 0.4335
shapiro.test(Rust$BubbleSort)       # W = 0.91152,  p-value = 8.002e-07
shapiro.test(Rust$Fibonacci)        # W = 0.90205,  p-value = 2.501e-07
shapiro.test(Rust$HeapSort)         # W = 0.90527,  p-value = 3.685e-07
shapiro.test(Rust$LinearSearch)     # W = 0.45708,  p-value < 2.2e-16
shapiro.test(Rust$MergeSort)        # W = 0.56386,  p-value < 2.2e-16
shapiro.test(Rust$NQueen24)         # W = 0.97237,  p-value = 0.01424
shapiro.test(Rust$NQueen27)         # W = 0.79029,  p-value = 8.382e-12
shapiro.test(Rust$PrimeNumber)      # W = 0.88119,  p-value = 2.399e-08
shapiro.test(Rust$SelectionSort)    # W = 0.88721,  p-value = 4.591e-08
shapiro.test(Rust$ShellSort)        # W = 0.97768,  p-value = 0.04334


# independent 2-group Mann-Whitney U Tests

# BinarySearch
wilcox.test(Rust$BinarySearch,AssemblyScript$BinarySearch)   # W = 680,    p-value < 2.2e-16
wilcox.test(Rust$BinarySearch,C$BinarySearch)                # W = 3991,   p-value = 2.427e-09
wilcox.test(Rust$BinarySearch,CPP$BinarySearch)              # W = 3514,   p-value = 7.219e-12

# BubbleSort
wilcox.test(Rust$BubbleSort,AssemblyScript$BubbleSort)       # W = 0,      p-value < 2.2e-16
wilcox.test(Rust$BubbleSort,C$BubbleSort)                    # W = 13774,  p-value < 2.2e-16
wilcox.test(Rust$BubbleSort,CPP$BubbleSort)                  # W = 14327,  p-value < 2.2e-16

# Fibonacci
wilcox.test(Rust$Fibonacci,AssemblyScript$Fibonacci)         # W = 0,      p-value < 2.2e-16
wilcox.test(Rust$Fibonacci,C$Fibonacci)                      # W = 2104,   p-value < 2.2e-16
wilcox.test(Rust$Fibonacci,CPP$Fibonacci)                    # W = 2832,   p-value = 4.606e-16

# HeapSort
wilcox.test(Rust$HeapSort,AssemblyScript$HeapSort)           # W = 0,      p-value < 2.2e-16
wilcox.test(Rust$HeapSort,C$HeapSort)                        # W = 11194,  p-value = 1.119e-13
wilcox.test(Rust$HeapSort,CPP$HeapSort)                      # W = 11289,  p-value = 2.901e-14

# LinearSearch
wilcox.test(Rust$LinearSearch,AssemblyScript$LinearSearch)   # W = 0,      p-value < 2.2e-16
wilcox.test(Rust$LinearSearch,C$LinearSearch)                # W = 120,    p-value < 2.2e-16
wilcox.test(Rust$LinearSearch,CPP$LinearSearch)              # W = 120,    p-value < 2.2e-16

# MergeSort
wilcox.test(Rust$MergeSort,AssemblyScript$MergeSort)         # W = 14038,  p-value < 2.2e-16
wilcox.test(Rust$MergeSort,C$MergeSort)                      # W = 14400,  p-value < 2.2e-16
wilcox.test(Rust$MergeSort,CPP$MergeSort)                    # W = 14400,  p-value < 2.2e-16

# NQueen24
wilcox.test(Rust$NQueen24,AssemblyScript$NQueen24)           # W = 0,      p-value < 2.2e-16
wilcox.test(Rust$NQueen24,C$NQueen24)                        # W = 8581,   p-value = 0.01026
wilcox.test(Rust$NQueen24,CPP$NQueen24)                      # W = 13619,  p-value < 2.2e-16

# NQueen27
wilcox.test(Rust$NQueen27,AssemblyScript$NQueen27)           # W = 0,      p-value < 2.2e-16
wilcox.test(Rust$NQueen27,C$NQueen27)                        # W = 11796,  p-value < 2.2e-16
wilcox.test(Rust$NQueen27,CPP$NQueen27)                      # W = 13528,  p-value < 2.2e-16

# PrimeNumber
wilcox.test(Rust$PrimeNumber,AssemblyScript$PrimeNumber)     # W = 0,      p-value < 2.2e-16
wilcox.test(Rust$PrimeNumber,C$PrimeNumber)                  # W = 14400,  p-value < 2.2e-16
wilcox.test(Rust$PrimeNumber,CPP$PrimeNumber)                # W = 14400,  p-value < 2.2e-16

# SelectionSort
wilcox.test(Rust$SelectionSort,AssemblyScript$SelectionSort) # W = 0,      p-value < 2.2e-16
wilcox.test(Rust$SelectionSort,C$SelectionSort)              # W = 14400,  p-value < 2.2e-16
wilcox.test(Rust$SelectionSort,CPP$SelectionSort)            # W = 14400,  p-value < 2.2e-16

# ShellSort
wilcox.test(Rust$ShellSort,AssemblyScript$ShellSort)         # W = 4802,   p-value = 8.265e-06
wilcox.test(Rust$ShellSort,C$ShellSort)                      # W = 13871,  p-value < 2.2e-16
wilcox.test(Rust$ShellSort,CPP$ShellSort)                    # W = 13835,  p-value < 2.2e-16


##############################################################################
##############################################################################
##############################################################################





\end{lstlisting}

\chapter{Distribution of Data}

The following figures represent the distribtion of the data obtained through statistical tests.

Figure \ref{fig:AppendixFibHist} shows the distribution of data for the Fibonacci sequence algorithm.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/fibonacci_histograms.PNG}
    \caption{Fibonacci}
    \label{fig:AppendixFibHist}
\end{figure}

\hfill \break
\hfill \break


Figure \ref{fig:AppendixNQueen24} shows the distribution of data for the NQueen24 algorithm.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/Histogram-NQueen24.PNG}
    \caption{NQueen24}
    \label{fig:AppendixNQueen24}
\end{figure}

Figure \ref{fig:AppendixNQueen27} shows the distribution of data for the NQueen27 algorithm.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/Histogram-NQueen27.PNG}
    \caption{NQueen27}
    \label{fig:AppendixNQueen27}
\end{figure}

\hfill \break


Figure \ref{fig:AppendixPrime} shows the distribution of data for the PrimeNumber algorithm.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/Histogram-PrimeNumber.PNG}
    \caption{PrimeNumber}
    \label{fig:AppendixPrime}
\end{figure}

Figure \ref{fig:AppendixBinarySearch} shows the distribution of data for the BinarySearch algorithm.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/Histogram-BinarySearch.PNG}
    \caption{BinarySearch}
    \label{fig:AppendixBinarySearch}
\end{figure}

Figure \ref{fig:AppendixLinearSearch} shows the distribution of data for the LinearSearch algorithm.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/Histogram-LinearSearch.PNG}
    \caption{LinearSearch}
    \label{fig:AppendixLinearSearch}
\end{figure}

Figure \ref{fig:AppendixBubbleSort} shows the distribution of data for the BubbleSort algorithm.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/Histogram-BubbleSort.PNG}
    \caption{BubbleSort}
    \label{fig:AppendixBubbleSort}
\end{figure}

\hfill \break

Figure \ref{fig:AppendixHeapSort} shows the distribution of data for the HeapSort algorithm.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/Histogram-HeapSort.PNG}
    \caption{HeapSort}
    \label{fig:AppendixHeapSort}
\end{figure}

Figure \ref{fig:AppendixMergeSort} shows the distribution of data for the MergeSort algorithm.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/Histogram-MergeSort.PNG}
    \caption{MergeSort}
    \label{fig:AppendixMergeSort}
\end{figure}

Figure \ref{fig:AppendixShellSort} shows the distribution of data for the ShellSort algorithm.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/Histogram-ShellSort.PNG}
    \caption{ShellSort}
    \label{fig:AppendixShellSort}
\end{figure}

Figure \ref{fig:AppendixSelectionSort} shows the distribution of data for the SelectionSort algorithm.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{images/Histogram-SelectionSort.PNG}
    \caption{SelectionSort}
    \label{fig:AppendixSelectionSort}
\end{figure}

\hfill \break

Figure \ref{fig:ASDist} represents the distribution of data for all the algorithms used in our experiments compiled from AssemblyScript.

\begin{figure}[H]
    \caption{AssemblyScript Distribtion}
    \label{fig:ASDist}
    \centering
        \subfloat[BinarySearch]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-BinarySearch.PNG}}\hfill
        \subfloat[BubbleSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-BubbleSort.PNG}}\hfill
        \subfloat[Fibonacci]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-Fibonacci.PNG}}\hfill
        \subfloat[HeapSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-HeapSort.PNG}}\hfill
        \subfloat[LinearSearch]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-LinearSearch.PNG}}\hfill
        \subfloat[MergeSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-MergeSort.PNG}}\hfill
        \subfloat[NQueen24]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-NQueen24.PNG}}\hfill
        \subfloat[NQueen27]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-NQueen27.PNG}}\hfill
        \subfloat[PrimaryNumber]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-PrimaryNumber.PNG}}\hfill
        \subfloat[SelectionSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-SelectionSort.PNG}}\hfill
        \subfloat[ShellSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/AssemblyScript-ShellSort.PNG}}\hfill
\end{figure}

\hfill \break

Figure \ref{fig:ASDist} represents the distribution of data for all the algorithms used in our experiments compiled from C.

\begin{figure}[H]
    \caption{C Distribtion}
    \label{fig:CDist}
    \centering
        \subfloat[BinarySearch]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/C-BinarySearch.PNG}}\hfill
        \subfloat[BubbleSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/C-BubbleSort.PNG}}\hfill
        \subfloat[Fibonacci]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/C-Fibonacci.PNG}}\hfill
        \subfloat[HeapSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/C-HeapSort.PNG}}\hfill
        \subfloat[LinearSearch]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/C-LinearSearch.PNG}}\hfill
        \subfloat[MergeSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/C-MergeSort.PNG}}\hfill
        \subfloat[NQueen24]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/C-NQueen24.PNG}}\hfill
        \subfloat[NQueen27]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/C-NQueen27.PNG}}\hfill
        \subfloat[PrimaryNumber]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/C-PrimeNumber.PNG}}\hfill
        \subfloat[SelectionSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/C-SelectionSort.PNG}}\hfill
        \subfloat[ShellSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/C-ShellSort.PNG}}\hfill
\end{figure}

\hfill \break

Figure \ref{fig:CPPDist} represents the distribution of data for all the algorithms used in our experiments compiled from C++.

\begin{figure}[H]
    \caption{C++ Distribtion}
    \label{fig:CPPDist}
    \centering
        \subfloat[BinarySearch]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/CPP-BinarySearch.PNG}}\hfill
        \subfloat[BubbleSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/CPP-BubbleSort.PNG}}\hfill
        \subfloat[Fibonacci]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/CPP-Fibonacci.PNG}}\hfill
        \subfloat[HeapSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/CPP-HeapSort.PNG}}\hfill
        \subfloat[LinearSearch]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/CPP-LinearSearch.PNG}}\hfill
        \subfloat[MergeSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/CPP-MergeSort.PNG}}\hfill
        \subfloat[NQueen24]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/CPP-NQueen24.PNG}}\hfill
        \subfloat[NQueen27]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/CPP-NQueen27.PNG}}\hfill
        \subfloat[PrimaryNumber]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/CPP-PrimeNumber.PNG}}\hfill
        \subfloat[SelectionSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/CPP-SelectionSort.PNG}}\hfill
        \subfloat[ShellSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/CPP-ShellSort.PNG}}\hfill
\end{figure}

\hfill \break

Figure \ref{fig:RustDist} represents the distribution of data for all the algorithms used in our experiments compiled from Rust.

\begin{figure}[H]
    \caption{Rust Distribtion}
    \label{fig:RustDist}
    \centering
        \subfloat[BinarySearch]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/Rust-BinarySearch.PNG}}\hfill
        \subfloat[BubbleSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/Rust-BubbleSort.PNG}}\hfill
        \subfloat[Fibonacci]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/Rust-Fibonacci.PNG}}\hfill
        \subfloat[HeapSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/Rust-HeapSort.PNG}}\hfill
        \subfloat[LinearSearch]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/Rust-LinearSearch.PNG}}\hfill
        \subfloat[MergeSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/Rust-MergeSort.PNG}}\hfill
        \subfloat[NQueen24]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/Rust-NQueen24.PNG}}\hfill
        \subfloat[NQueen27]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/Rust-NQueen27.PNG}}\hfill
        \subfloat[PrimaryNumber]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/Rust-PrimeNumber.PNG}}\hfill
        \subfloat[SelectionSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/Rust-SelectionSort.PNG}}\hfill
        \subfloat[ShellSort]{\label{sfig:a}\includegraphics[width=.3\textwidth]{images/Rust-ShellSort.PNG}}\hfill
\end{figure}

\end{document}